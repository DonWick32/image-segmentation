{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 63\n"
     ]
    }
   ],
   "source": [
    "# !sudo mount -t drvfs E: /mnt/g\n",
    "# mogrify -format jpg *.png && rm *.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"data/raw\"\n",
    "is_manual_annotation = False\n",
    "perform_tta = True\n",
    "\n",
    "num_pos_points_per_tool = 5\n",
    "num_neg_points_per_tool = 5\n",
    "override_annotations = True\n",
    "\n",
    "save_img = True\n",
    "\n",
    "annotation_dir = \"data/annotations/\" + \"manual\" if is_manual_annotation else \"data/annotations/\" + \"auto\"\n",
    "\n",
    "masks_dir = \"data/masks\"\n",
    "results_dir = \"data/results\"\n",
    "log_dir =  \"logs\"\n",
    "val_path = dataset_path + \"/SegSTRONGC_val/val\"\n",
    "test_path = dataset_path + \"/SegSTRONGC_test/test\"\n",
    "train_path = dataset_path + \"/SegSTRONGC_train\"\n",
    "models = [\"sam2.1_hiera_base_plus\", \"yolo11x-seg\"]\n",
    "model = models[0]\n",
    "\n",
    "val_dirs = [val_path + \"/\" + domain for domain in os.listdir(val_path)]\n",
    "# test_dirs = [test_path + \"/\" + domain for domain in os.listdir(test_path)]\n",
    "# train_dirs = [train_path + \"/\" + domain for domain in os.listdir(train_path)]\n",
    "\n",
    "\n",
    "sub_dirs = [path + \"/\" + sub_dir for path in val_dirs for sub_dir in os.listdir(path)]\n",
    "# test_sub_dirs = [path + \"/\" + sub_dir for path in test_dirs for sub_dir in os.listdir(path)]\n",
    "# train_sub_dirs = [path + \"/\" + sub_dir for path in train_dirs for sub_dir in os.listdir(path)]\n",
    "\n",
    "# ground truth is 'ground_truth'\n",
    "domains = ['bg_change', 'blood', 'low_brightness', 'regular', 'smoke']\n",
    "test_domains = ['bg_change', 'blood', 'low_brightness', 'regular', 'smoke']\n",
    "train_domains = ['regular']\n",
    "\n",
    "checkpoints = {\n",
    "    \"sam2.1_hiera_base_plus\": \"checkpoints/sam2.1_hiera_base_plus.pt\",\n",
    "    \"yolo11x-seg\": \"checkpoints/yolo11x-seg.pt\"\n",
    "}\n",
    "\n",
    "model_cfgs = {\n",
    "    \"sam2.1_hiera_base_plus\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "    \"yolo11x-seg\": None\n",
    "}\n",
    "\n",
    "num_images_per_domain = 300\n",
    "checkpoint = checkpoints[model]\n",
    "model_cfg = model_cfgs[model]\n",
    "\n",
    "logging.basicConfig(filename=log_dir + f'/{model}.log', level=logging.INFO, format='%(asctime)s - %(message)s', filemode='w')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/raw/SegSTRONGC_val/val/bg_change/left/0.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/1.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/2.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/3.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/4.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/5.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/6.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/7.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/8.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/9.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/10.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/11.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/12.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/13.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/14.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/15.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/16.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/17.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/18.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/19.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/20.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/21.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/22.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/23.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/24.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/25.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/26.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/27.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/28.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/29.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/30.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/31.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/32.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/33.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/34.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/35.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/36.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/37.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/38.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/39.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/40.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/41.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/42.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/43.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/44.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/45.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/46.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/47.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/48.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/49.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/50.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/51.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/52.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/53.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/54.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/55.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/56.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/57.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/58.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/59.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/60.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/61.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/62.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/63.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/64.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/65.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/66.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/67.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/68.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/69.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/70.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/71.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/72.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/73.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/74.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/75.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/76.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/77.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/78.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/79.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/80.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/81.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/82.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/83.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/84.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/85.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/86.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/87.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/88.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/89.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/90.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/91.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/92.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/93.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/94.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/95.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/96.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/97.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/98.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/99.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/100.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/101.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/102.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/103.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/104.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/105.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/106.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/107.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/108.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/109.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/110.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/111.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/112.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/113.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/114.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/115.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/116.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/117.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/118.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/119.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/120.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/121.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/122.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/123.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/124.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/125.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/126.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/127.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/128.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/129.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/130.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/131.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/132.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/133.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/134.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/135.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/136.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/137.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/138.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/139.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/140.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/141.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/142.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/143.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/144.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/145.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/146.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/147.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/148.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/149.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/150.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/151.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/152.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/153.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/154.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/155.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/156.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/157.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/158.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/159.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/160.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/161.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/162.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/163.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/164.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/165.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/166.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/167.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/168.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/169.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/170.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/171.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/172.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/173.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/174.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/175.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/176.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/177.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/178.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/179.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/180.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/181.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/182.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/183.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/184.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/185.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/186.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/187.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/188.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/189.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/190.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/191.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/192.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/193.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/194.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/195.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/196.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/197.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/198.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/199.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/200.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/201.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/202.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/203.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/204.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/205.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/206.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/207.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/208.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/209.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/210.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/211.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/212.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/213.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/214.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/215.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/216.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/217.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/218.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/219.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/220.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/221.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/222.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/223.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/224.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/225.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/226.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/227.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/228.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/229.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/230.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/231.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/232.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/233.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/234.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/235.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/236.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/237.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/238.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/239.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/240.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/241.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/242.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/243.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/244.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/245.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/246.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/247.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/248.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/249.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/250.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/251.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/252.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/253.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/254.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/255.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/256.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/257.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/258.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/259.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/260.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/261.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/262.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/263.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/264.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/265.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/266.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/267.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/268.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/269.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/270.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/271.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/272.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/273.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/274.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/275.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/276.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/277.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/278.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/279.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/280.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/281.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/282.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/283.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/284.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/285.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/286.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/287.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/288.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/289.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/290.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/291.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/292.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/293.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/294.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/295.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/296.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/297.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/298.png',\n",
       " 'data/raw/SegSTRONGC_val/val/bg_change/left/299.png']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_image_paths(path, domain, is_left, num_images=300):\n",
    "    stereo_dir = \"left\" if is_left else \"right\"\n",
    "    image_paths = []\n",
    "\n",
    "    for i in range(num_images):\n",
    "        image_paths.append(path + \"/\" + domain + \"/\" + stereo_dir + \"/\" + str(i) + \".png\")\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "get_image_paths(val_path, \"bg_change\", True, num_images=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(TP, FP, FN):\n",
    "    return TP / (TP + FP + FN)\n",
    "\n",
    "def calculate_dsc(TP, FP, FN):\n",
    "    return 2 * TP / (2 * TP + FP + FN)\n",
    "\n",
    "def calculate_miou(pred_masks, gt_masks):\n",
    "    ious = []\n",
    "    for i in range(len(pred_masks)):\n",
    "        TP = np.logical_and(pred_masks[i], gt_masks[i])\n",
    "        FP = np.logical_and(pred_masks[i], np.logical_not(gt_masks[i]))\n",
    "        FN = np.logical_and(np.logical_not(pred_masks[i]), gt_masks[i])\n",
    "\n",
    "        iou = calculate_iou(np.sum(TP), np.sum(FP), np.sum(FN))\n",
    "        ious.append(iou)\n",
    "    \n",
    "    return np.mean(ious)\n",
    "\n",
    "def calculate_mdsc(pred_masks, gt_masks):\n",
    "    dscs = []\n",
    "    for i in range(len(pred_masks)):\n",
    "        TP = np.logical_and(pred_masks[i], gt_masks[i])\n",
    "        FP = np.logical_and(pred_masks[i], np.logical_not(gt_masks[i]))\n",
    "        FN = np.logical_and(np.logical_not(pred_masks[i]), gt_masks[i])\n",
    "\n",
    "        dsc = calculate_dsc(np.sum(TP), np.sum(FP), np.sum(FN))\n",
    "        dscs.append(dsc)\n",
    "    \n",
    "    return np.mean(dscs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py:128\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3308\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3305\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_trap:\n\u001b[1;32m   3306\u001b[0m     \u001b[38;5;66;03m# Compile to bytecode\u001b[39;00m\n\u001b[1;32m   3307\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3308\u001b[0m         code_ast \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mast_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3309\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_exceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3310\u001b[0m         etype, value, tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/compilerop.py:86\u001b[0m, in \u001b[0;36mCachingCompiler.ast_parse\u001b[0;34m(self, source, filename, symbol)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mast_parse\u001b[39m(\u001b[38;5;28mself\u001b[39m, source, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<unknown>\u001b[39m\u001b[38;5;124m'\u001b[39m, symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexec\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse code to an AST with the current compiler flags active.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    Arguments are exactly the same as ast.parse (in the standard library),\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    and are passed to the built-in compile function.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPyCF_ONLY_AST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py:128\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3286\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_history:\n\u001b[1;32m   3285\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3286\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n\u001b[1;32m   3288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(cell, raw_cell)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/history.py:792\u001b[0m, in \u001b[0;36mHistoryManager.store_inputs\u001b[0;34m(self, line_num, source, source_raw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_hist_parsed\u001b[38;5;241m.\u001b[39mappend(source)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_hist_raw\u001b[38;5;241m.\u001b[39mappend(source_raw)\n\u001b[0;32m--> 792\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb_input_cache_lock\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb_input_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_raw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Trigger to flush cache and write to DB.\u001b[39;49;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def manual_annotate(frame_path):\n",
    "    annotations = {0: []}\n",
    "    current_tool = 0\n",
    "    is_positive = True\n",
    "\n",
    "    window_name = \"Manual Annotation of Frame -\" + str(frame_path)\n",
    "    cv2.namedWindow(window_name)\n",
    "\n",
    "    def handle_mouse_click(event, x, y, flags, params):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            annotations[current_tool].append({\n",
    "                \"x\": x,\n",
    "                \"y\": y,\n",
    "                \"label\": 1 if is_positive else 0\n",
    "            })\n",
    "\n",
    "            if is_positive:\n",
    "                cv2.circle(frame, (x, y), 10, (0, 255, 0), -1)\n",
    "            else:\n",
    "                cv2.circle(frame, (x, y), 10, (0, 0, 255), -1)\n",
    "\n",
    "        cv2.imshow(window_name, frame)\n",
    "\n",
    "    cv2.setMouseCallback(window_name, handle_mouse_click)\n",
    "    frame = cv2.imread(frame_path)\n",
    "    original_frame = frame.copy()\n",
    "\n",
    "    while True:\n",
    "        frame = original_frame.copy()\n",
    "        for tool in annotations:\n",
    "            for annotation in annotations[tool]:\n",
    "                if annotation[\"label\"] == 1:\n",
    "                    cv2.circle(frame, (annotation[\"x\"], annotation[\"y\"]), 10, (0, 255, 0), -1)\n",
    "                else:\n",
    "                    cv2.circle(frame, (annotation[\"x\"], annotation[\"y\"]), 10, (0, 0, 255), -1)\n",
    "                \n",
    "        display_text = f\"Tool: {current_tool}, Mode: {'Positive' if is_positive else 'Negative'}\"\n",
    "        cv2.putText(frame, display_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(window_name, frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord(\"n\"):\n",
    "            current_tool += 1\n",
    "            is_positive = True\n",
    "            annotations[current_tool] = []\n",
    "        elif key == ord(\"p\"):\n",
    "            is_positive = not is_positive\n",
    "        elif key == ord(\"c\"):\n",
    "            annotations[current_tool] = []\n",
    "        elif key == ord(\"s\"):\n",
    "            break\n",
    "\n",
    "    cv2.destroyWindow(window_name)\n",
    "    return annotations\n",
    "\n",
    "def auto_annotate(frame_path):\n",
    "    ground_truth_mask_path = None\n",
    "    annotations = {}\n",
    "    # annotations[current_tool].append({\n",
    "    #             \"x\": x,\n",
    "    #             \"y\": y,\n",
    "    #             \"label\": 1 if is_positive else 0\n",
    "    #         })\n",
    "\n",
    "    for domain in domains:\n",
    "        if domain in frame_path:\n",
    "            ground_truth_mask_path = frame_path.replace(domain, \"ground_truth\")\n",
    "            break\n",
    "    \n",
    "    if ground_truth_mask_path is None:\n",
    "        raise ValueError(\"Ground truth path not found.\")\n",
    "    else:\n",
    "        ground_truth_mask = cv2.imread(ground_truth_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        ground_truth_mask = (ground_truth_mask > 0).astype(np.bool_)\n",
    "        _, labels = cv2.connectedComponents(ground_truth_mask.astype(np.uint8))\n",
    "\n",
    "        # get unique labels, and get count for each label, then sort by count in descending order\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        sorted_labels = unique_labels[np.argsort(-counts)]\n",
    "\n",
    "        background_label = sorted_labels[0]\n",
    "        first_tool_label = sorted_labels[1]\n",
    "\n",
    "        if len(sorted_labels) > 2:\n",
    "            second_tool_label = sorted_labels[2]\n",
    "        else:\n",
    "            second_tool_label = -1\n",
    "\n",
    "        #for each tool, get the centroid, and add num_auto_points - 1 sampled random points\n",
    "        annotations[0] = []\n",
    "        annotations[1] = []\n",
    "\n",
    "        for label, obj_id in [(first_tool_label, 0), (second_tool_label, 1)]:\n",
    "\n",
    "            if label == -1:\n",
    "                continue\n",
    "            \n",
    "            mask = labels == label\n",
    "            mask = mask.astype(np.uint8)\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            contour = contours[0]\n",
    "            M = cv2.moments(contour)\n",
    "            cx = int(M['m10'] / M['m00'])\n",
    "            cy = int(M['m01'] / M['m00'])\n",
    "\n",
    "            annotations[obj_id].append({\n",
    "                \"x\": cx,\n",
    "                \"y\": cy,\n",
    "                \"label\": 1\n",
    "            })\n",
    "\n",
    "            label_indices = np.where(labels == label)\n",
    "            random_indices = np.random.choice(len(label_indices[0]), num_pos_points_per_tool - 1, replace=False)\n",
    "            for i in random_indices:\n",
    "                x = int(label_indices[1][i])\n",
    "                y = int(label_indices[0][i])\n",
    "                if ground_truth_mask[y, x]:\n",
    "                    annotations[obj_id].append({\n",
    "                        \"x\": x,\n",
    "                        \"y\": y,\n",
    "                        \"label\": 1\n",
    "                    })\n",
    "\n",
    "            label_indices = np.where(labels == background_label)\n",
    "            # print(\"HIII \", label_indices)\n",
    "            random_indices = np.random.choice(len(label_indices[0]), num_neg_points_per_tool, replace=False)\n",
    "            for i in random_indices:\n",
    "                x = int(label_indices[1][i])\n",
    "                y = int(label_indices[0][i])\n",
    "                if not ground_truth_mask[y, x]:\n",
    "                    annotations[obj_id].append({\n",
    "                        \"x\": x,\n",
    "                        \"y\": y,\n",
    "                        \"label\": 0\n",
    "                    })\n",
    "\n",
    "\n",
    "    # place these points on the frame and display once to verify\n",
    "    # frame = cv2.imread(frame_path)\n",
    "    # for tool in annotations:\n",
    "    #     for annotation in annotations[tool]:\n",
    "    #         cv2.circle(frame, (annotation[\"x\"], annotation[\"y\"]), 10, (0, 255, 0), -1)\n",
    "\n",
    "    # window_name = \"Auto Annotation of Frame -\" + str(frame_path)\n",
    "    # cv2.namedWindow(window_name)\n",
    "    # cv2.imshow(window_name, frame)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyWindow(window_name)\n",
    "\n",
    "    # plt.imshow(frame)\n",
    "    # plt.show()\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def manual_annotate_first_frames(sub_dirs, domains, split):\n",
    "    print(f\"Total number of subdirs for {split}: {len(sub_dirs)}\")\n",
    "    for sub_dir in sub_dirs:\n",
    "        for domain in domains:\n",
    "            left_video_frames_path = sub_dir + \"/\" + domain + \"/left\"\n",
    "            right_video_frames_path = sub_dir + \"/\" + domain + \"/right\"\n",
    "\n",
    "            first_left_frame = left_video_frames_path + \"/0.png\"\n",
    "            first_right_frame = right_video_frames_path + \"/0.png\"\n",
    "\n",
    "            last_left_frame = left_video_frames_path + \"/299.png\"\n",
    "            last_right_frame = right_video_frames_path + \"/299.png\"\n",
    "\n",
    "            if is_manual_annotation:\n",
    "                left_annotations = manual_annotate(first_left_frame)\n",
    "                right_annotations = manual_annotate(first_right_frame)\n",
    "\n",
    "                left_reverse_annotations = manual_annotate(last_left_frame)\n",
    "                right_reverse_annotations = manual_annotate(last_right_frame)\n",
    "            else:\n",
    "                left_annotations = auto_annotate(first_left_frame)\n",
    "                right_annotations = auto_annotate(first_right_frame)\n",
    "\n",
    "                left_reverse_annotations = auto_annotate(last_left_frame)\n",
    "                right_reverse_annotations = auto_annotate(last_right_frame)\n",
    "\n",
    "            annotation_file = annotation_dir + f\"/{split}.json\"\n",
    "            if os.path.exists(annotation_file):\n",
    "                with open(annotation_file, \"r\") as f:\n",
    "                    all_annotations = json.load(f)\n",
    "            else:\n",
    "                all_annotations = {}\n",
    "\n",
    "            # print(left_annotations)\n",
    "            # print(right_annotations)\n",
    "            all_annotations[sub_dir + \"/\" + domain + \"/left\"] = left_annotations\n",
    "            all_annotations[sub_dir + \"/\" + domain + \"/right\"] = right_annotations\n",
    "\n",
    "            # print(all_annotations)\n",
    "\n",
    "            with open(annotation_file, \"w\") as f:\n",
    "                json.dump(all_annotations, f)\n",
    "\n",
    "            print(f\"Domain annotated: {first_left_frame}\")\n",
    "\n",
    "            annotation_file = annotation_dir + f\"/{split}_reverse.json\"\n",
    "            if os.path.exists(annotation_file):\n",
    "                with open(annotation_file, \"r\") as f:\n",
    "                    all_annotations = json.load(f)\n",
    "            else:\n",
    "                all_annotations = {}\n",
    "\n",
    "            all_annotations[sub_dir + \"/\" + domain + \"/left\"] = left_reverse_annotations\n",
    "            all_annotations[sub_dir + \"/\" + domain + \"/right\"] = right_reverse_annotations\n",
    "\n",
    "            with open(annotation_file, \"w\") as f:\n",
    "                json.dump(all_annotations, f)\n",
    "\n",
    "            print(f\"Reverse Domain annotated: {last_left_frame}\")\n",
    "        print(f\"Subdir annotated: {sub_dir}\")\n",
    "\n",
    "if override_annotations:\n",
    "    manual_annotate_first_frames(sub_dirs, domains, \"val\")\n",
    "    print(\"Split annotated: val\")\n",
    "    # manual_annotate_first_frames(test_sub_dirs, test_domains, \"test\")\n",
    "    # print(\"Split annotated: test\")\n",
    "    # manual_annotate_first_frames(train_sub_dirs, train_domains, \"train\")\n",
    "    # print(\"Split annotated: train\")\n",
    "\n",
    "else:\n",
    "    print(\"Annotations already exist. Skipping annotation process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(inference_model, frames_path, split, is_reverse, forward_pass_path=None):\n",
    "    mask_storage_data = {}\n",
    "    predicted_masks = []\n",
    "    \n",
    "    if inference_model == \"sam2.1_hiera_base_plus\":\n",
    "        print(f\"Loading annotations for split: {split}\")\n",
    "        try:\n",
    "            if not is_reverse:\n",
    "                with open(annotation_dir + f\"/{split}.json\", \"r\") as f:\n",
    "                    annotations = json.load(f)\n",
    "                print(f\"Successfully loaded annotations for {len(annotations)} items\")\n",
    "            else:\n",
    "                with open(annotation_dir + f\"/{split}_reverse.json\", \"r\") as f:\n",
    "                    annotations = json.load(f)\n",
    "                print(f\"Successfully loaded reverse annotations for {len(annotations)} items\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading annotations: {e}\")\n",
    "            logger.error(f\"Error loading annotations for split {split}: {e}\")\n",
    "            return\n",
    "\n",
    "        if annotations is None:\n",
    "            print(\"No annotations found for split\", split)\n",
    "            logger.warning(f\"No annotations found for split {split}\")\n",
    "            return None, None\n",
    "        \n",
    "        start = time.time()\n",
    "        print(f\"Initializing SAM for video...\")\n",
    "        sam2_predictor = build_sam2_video_predictor(model_cfg, checkpoint, device=device)\n",
    "\n",
    "        inference_state = sam2_predictor.init_state(\n",
    "            video_path = frames_path,\n",
    "        )\n",
    "        end = time.time()\n",
    "        print(f\"Initialization took {end - start:.2f} seconds.\")\n",
    "        logger.info(f\"SAM initialization for {forward_pass_path} took {end - start:.2f} seconds.\")\n",
    "\n",
    "        current_annotations = annotations[forward_pass_path]\n",
    "        print(f\"Found {len(current_annotations)} objects with annotations.\")\n",
    "        logger.info(f\"Processing {len(current_annotations)} objects with annotations for {forward_pass_path}\")\n",
    "        \n",
    "        n_points = 0\n",
    "        for object in tqdm(current_annotations, desc=f\"Processing annotations for objects\"):\n",
    "            n_points += len(current_annotations[object])\n",
    "            object_points = []\n",
    "            object_labels = []\n",
    "            for annotation in current_annotations[object]:\n",
    "                object_points.append([int(annotation[\"x\"]), int(annotation[\"y\"])])\n",
    "                object_labels.append(annotation[\"label\"])\n",
    "\n",
    "            points = np.array(object_points, np.float32)\n",
    "            labels = np.array(object_labels, np.int32)\n",
    "\n",
    "            if points.shape[0] == 0:\n",
    "                continue\n",
    "            \n",
    "            _, out_obj_ids, out_mask_logits = sam2_predictor.add_new_points_or_box(\n",
    "                inference_state = inference_state,\n",
    "                frame_idx = 0,\n",
    "                obj_id = int(object),\n",
    "                points = points,\n",
    "                labels = labels,\n",
    "            )\n",
    "        print(f\"Added {n_points} annotation points across all objects.\")\n",
    "        logger.info(f\"Added {n_points} annotation points across all objects for {forward_pass_path}\")\n",
    "\n",
    "        print(\"Starting mask propagation...\")\n",
    "        start = time.time()\n",
    "        video_segments = {}\n",
    "\n",
    "        n_frames = 0\n",
    "        for out_frame_idx, out_obj_ids, out_mask_logits in sam2_predictor.propagate_in_video(inference_state):\n",
    "            n_frames += 1\n",
    "            video_segments[out_frame_idx] = {\n",
    "                out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "                for i, out_obj_id in enumerate(out_obj_ids)\n",
    "            }\n",
    "\n",
    "        end = time.time()\n",
    "        prop_time = end - start\n",
    "\n",
    "        print(f\"Mask propagation completed in {prop_time:.2f} seconds.\")\n",
    "        logger.info(f\"Mask propagation for {forward_pass_path} took {prop_time:.2f} seconds.\")\n",
    "\n",
    "        print(\"Processing predicted masks...\")\n",
    "        for frame_idx, obj_dict in tqdm(video_segments.items(), desc=\"Processing video frames\"):\n",
    "            # it should have shape (1080, 1920)\n",
    "            # mask_storage_data[frame_idx] = []\n",
    "            overall_mask = np.zeros((1080, 1920), dtype=bool)\n",
    "\n",
    "            for obj_id, mask_array in obj_dict.items():\n",
    "                # mask_storage_data[frame_idx].append({\n",
    "                #     obj_id: mask_array\n",
    "                # })\n",
    "                \n",
    "                overall_mask = np.logical_or(overall_mask, mask_array.squeeze())\n",
    "\n",
    "            predicted_masks.append(overall_mask)\n",
    "            mask_storage_data[frame_idx] = overall_mask\n",
    "\n",
    "        sam2_predictor.reset_state(inference_state)\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"Empied CUDA cache.\")\n",
    "        print(\"SAM state reset.\")\n",
    "    elif inference_model == \"yolo11x-seg\":\n",
    "        start = time.time()\n",
    "        print(f\"Initializing Yolo for video...\")\n",
    "        yolo_model = YOLO(checkpoint)\n",
    "        end = time.time()\n",
    "        print(f\"Initialization took {end - start:.2f} seconds.\")\n",
    "        logger.info(f\"Yolo initialization for {frames_path} took {end - start:.2f} seconds.\")\n",
    "\n",
    "        print(\"Starting mask propagation...\")\n",
    "        start = time.time()\n",
    "\n",
    "        total_images = len(os.listdir(frames_path))\n",
    "        print(f\"Total images in video: {total_images}\")\n",
    "        logger.info(f\"Total images in video: {total_images}\")\n",
    "\n",
    "        video_result = []\n",
    "\n",
    "        for i in tqdm(range(total_images), desc=\"Propagating video frames\"):\n",
    "            frame_path = frames_path + f\"/{i}.png\"\n",
    "            # video_result.append(yolo_model(frame_path, max_det = 2))\n",
    "            video_result.append(yolo_model(frame_path))\n",
    "\n",
    "        end = time.time()\n",
    "        prop_time = end - start\n",
    "\n",
    "        print(f\"Mask propagation completed in {prop_time:.2f} seconds.\")\n",
    "        logger.info(f\"Mask propagation for {frames_path} took {prop_time:.2f} seconds.\")\n",
    "\n",
    "        print(\"Processing predicted masks...\")\n",
    "\n",
    "        for frame_idx, frame_result in tqdm(enumerate(video_result), desc=\"Processing video frames\"):\n",
    "            # it should have shape (1080, 1920)\n",
    "            # mask_storage_data[frame_idx] = []\n",
    "            overall_mask = np.zeros((1080, 1920), dtype=bool)\n",
    "\n",
    "            for result in frame_result:\n",
    "                if result.masks is None:\n",
    "                    # mask_storage_data[frame_idx].append({\n",
    "                    #     0: overall_mask\n",
    "                    # })\n",
    "                    continue\n",
    "\n",
    "                for mask_id, mask in enumerate(result.masks.data):\n",
    "                    mask_np = mask.cpu().numpy()\n",
    "                    reshaped_mask = cv2.resize(mask_np, (1920, 1080), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                    # mask_storage_data[frame_idx].append({\n",
    "                    #     mask_id: reshaped_mask\n",
    "                    # })\n",
    "\n",
    "                    overall_mask = np.logical_or(overall_mask, reshaped_mask)\n",
    "\n",
    "            predicted_masks.append(overall_mask)\n",
    "            mask_storage_data[frame_idx] = overall_mask\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name.\")\n",
    "    \n",
    "    return mask_storage_data, predicted_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running inference for split: val\n",
      "================================================================================\n",
      "Processing 3 sub-directories and 5 domains\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sub-directories:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Processing sub-directory: data/raw/SegSTRONGC_val/val/1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing domain: bg_change\n",
      "Processing left camera video...\n",
      "==================================================\n",
      "Processing video: data/raw/SegSTRONGC_val/val/1/2/bg_change/left\n",
      "Domain: bg_change, Split: val, Camera: left\n",
      "Loading annotations for split: val\n",
      "Successfully loaded annotations for 30 items\n",
      "Initializing SAM for video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "frame loading (JPEG): 100%|| 300/300 [00:30<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization took 58.92 seconds.\n",
      "Found 2 objects with annotations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "Processing annotations for objects: 100%|| 2/2 [00:02<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 20 annotation points across all objects.\n",
      "Starting mask propagation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "propagate in video:  15%|        | 45/300 [06:47<38:30,  9.06s/it]\n",
      "Processing domains:   0%|          | 0/5 [07:48<?, ?it/s]\n",
      "Processing sub-directories:   0%|          | 0/3 [07:48<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 261\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved to CSV file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    258\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved to CSV file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 261\u001b[0m \u001b[43mprocess_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomains\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# process_split(test_sub_dirs, test_domains, \"test\")\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# process_split(train_sub_dirs, train_domains, \"train\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 112\u001b[0m, in \u001b[0;36mprocess_split\u001b[0;34m(sub_dirs, domains, split)\u001b[0m\n\u001b[1;32m    109\u001b[0m right_video_frames_path \u001b[38;5;241m=\u001b[39m sub_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m domain \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/right\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing left camera video...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m left_miou, left_msdc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_video_frames_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing right camera video...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m right_miou, right_msdc \u001b[38;5;241m=\u001b[39m process_video(right_video_frames_path, sub_dir, domain, split, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m(frames_path, sub_dir, domain, split, is_left)\u001b[0m\n\u001b[1;32m     11\u001b[0m ground_truth_masks_path \u001b[38;5;241m=\u001b[39m sub_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/ground_truth/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m stereo_dir\n\u001b[1;32m     13\u001b[0m overall_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 14\u001b[0m mask_storage_data, predicted_masks \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(predicted_masks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m masks for evaluation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(predicted_masks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m masks for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframes_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 66\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(inference_model, frames_path, split)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m out_frame_idx, out_obj_ids, out_mask_logits \u001b[38;5;129;01min\u001b[39;00m sam2_predictor\u001b[38;5;241m.\u001b[39mpropagate_in_video(inference_state):\n\u001b[1;32m     64\u001b[0m     n_frames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     65\u001b[0m     video_segments[out_frame_idx] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 66\u001b[0m         out_obj_id: \u001b[43m(\u001b[49m\u001b[43mout_mask_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, out_obj_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(out_obj_ids)\n\u001b[1;32m     68\u001b[0m     }\n\u001b[1;32m     70\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     71\u001b[0m prop_time \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def process_video(frames_path, sub_dir, domain, split, is_left):\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Processing video: {frames_path}\")\n",
    "    print(f\"Domain: {domain}, Split: {split}, Camera: {'left' if is_left else 'right'}\")\n",
    "    logger.info(f\"Processing video: {frames_path} (Domain: {domain}, Split: {split}, Camera: {'left' if is_left else 'right'})\")\n",
    "    stereo_dir = \"left\" if is_left else \"right\"\n",
    "    ground_truth_masks_path = sub_dir + \"/ground_truth/\" + stereo_dir\n",
    "\n",
    "    overall_start = time.time()\n",
    "    mask_storage_data, predicted_masks = run_inference(model, frames_path, split, False, frames_path)\n",
    "\n",
    "    if perform_tta:\n",
    "        temp_video_frames_path = \"data/temp\"\n",
    "        if not os.path.exists(temp_video_frames_path):\n",
    "            os.makedirs(temp_video_frames_path)\n",
    "\n",
    "        total_files = len(os.listdir(frames_path))\n",
    "        for filename in os.listdir(frames_path):\n",
    "            if filename.endswith(\".png\"):\n",
    "                original_index = int(filename.split('.')[0])\n",
    "                new_index = total_files - 1 - original_index\n",
    "                new_filename = f\"{new_index}.png\"\n",
    "                shutil.copy(os.path.join(frames_path, filename), os.path.join(temp_video_frames_path, new_filename))\n",
    "\n",
    "        reverse_mask_storage_data, predicted_reverse_masks = run_inference(model, temp_video_frames_path, split, True, frames_path)\n",
    "        predicted_reverse_masks = predicted_reverse_masks[::-1]\n",
    "\n",
    "        if save_img:\n",
    "            save_dir = f\"data/results/{model}/visualizations\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            for i in range(len(predicted_masks)):\n",
    "                # Get frame path and load original image\n",
    "                frame_path = os.path.join(frames_path, f\"{i}.png\")\n",
    "                original_img = cv2.imread(frame_path)\n",
    "                \n",
    "                # Load ground truth mask\n",
    "                gt_mask = cv2.imread(os.path.join(ground_truth_masks_path, f\"{i}.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "                gt_mask = (gt_mask > 0).astype(np.uint8) * 255\n",
    "                \n",
    "                # Convert predicted masks to uint8\n",
    "                forward_mask = predicted_masks[i].astype(np.uint8) * 255\n",
    "                reverse_mask = predicted_reverse_masks[i].astype(np.uint8) * 255\n",
    "                \n",
    "                # Create visualization grid\n",
    "                h, w = original_img.shape[:2]\n",
    "                grid = np.zeros((h*2, w*2, 3), dtype=np.uint8)\n",
    "                \n",
    "                # Place images in grid\n",
    "                grid[:h, :w] = original_img  # Original\n",
    "                grid[:h, w:] = cv2.cvtColor(gt_mask, cv2.COLOR_GRAY2BGR)  # Ground truth\n",
    "                grid[h:, :w] = cv2.cvtColor(forward_mask, cv2.COLOR_GRAY2BGR)  # Forward mask\n",
    "                grid[h:, w:] = cv2.cvtColor(reverse_mask, cv2.COLOR_GRAY2BGR)  # Reverse mask\n",
    "                \n",
    "                # Add labels\n",
    "                labels = ['Original', 'Ground Truth', 'Forward Mask', 'Reverse Mask']\n",
    "                positions = [(10, 30), (w+10, 30), (10, h+30), (w+10, h+30)]\n",
    "                \n",
    "                for label, pos in zip(labels, positions):\n",
    "                    cv2.putText(grid, label, pos, cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "                \n",
    "                # Save the grid\n",
    "                save_path = os.path.join(save_dir, f\"frame_{i:04d}.png\")\n",
    "                cv2.imwrite(save_path, grid)\n",
    "                \n",
    "                print(f\"Saved visualization images to {save_dir}\")\n",
    "\n",
    "        save_img = False\n",
    "\n",
    "        for i in range(len(predicted_masks)):\n",
    "            predicted_masks[i] = np.logical_or(predicted_masks[i], predicted_reverse_masks[i])\n",
    "\n",
    "        shutil.rmtree(temp_video_frames_path)\n",
    "        print(f\"Applied TTA to {len(predicted_masks)} masks.\")\n",
    "    \n",
    "    print(f\"Generated {len(predicted_masks)} masks for evaluation.\")\n",
    "    logger.info(f\"Generated {len(predicted_masks)} masks for {frames_path}\")\n",
    "\n",
    "    # Save the masks\n",
    "    masks_split_dir = masks_dir + f\"/{model}\" + f\"/{split}\"\n",
    "    if not os.path.exists(masks_split_dir):\n",
    "        os.makedirs(masks_split_dir)\n",
    "\n",
    "    masks_file = masks_split_dir + f\"/{frames_path.replace('/', '-')}.pkl\"\n",
    "\n",
    "    # data = {}\n",
    "    # data[frames_path] = mask_storage_data\n",
    "\n",
    "    # with open(masks_file, \"wb\") as f:\n",
    "    #     pickle.dump(data, f)\n",
    "    # print(f\"Masks saved to {masks_file}\")\n",
    "\n",
    "    print(\"Loading ground truth masks for evaluation...\")\n",
    "    ground_truth_masks = []\n",
    "    for i in range(len(predicted_masks)):\n",
    "        ground_truth_mask = cv2.imread(ground_truth_masks_path + \"/\" + str(i) + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "        ground_truth_mask = (ground_truth_mask > 0).astype(np.bool_)\n",
    "        ground_truth_masks.append(ground_truth_mask)\n",
    "    print(f\"Loaded {len(ground_truth_masks)} ground truth masks.\")\n",
    "\n",
    "    print(\"Calculating evaluation metrics...\")\n",
    "    start = time.time()\n",
    "    miou = calculate_miou(predicted_masks, ground_truth_masks)\n",
    "    mdsc = calculate_mdsc(predicted_masks, ground_truth_masks)\n",
    "    end = time.time()\n",
    "    eval_time = end - start\n",
    "    print(f\"Time taken for metrics calculation: {eval_time:.2f} seconds.\")\n",
    "    logger.info(f\"Time taken for metrics calculation: {eval_time:.2f} seconds.\")\n",
    "\n",
    "    print(f\"Mean IoU for {sub_dir}/{domain}/{stereo_dir}: {miou:.4f}\")\n",
    "    print(f\"Mean DSC for {sub_dir}/{domain}/{stereo_dir}: {mdsc:.4f}\")\n",
    "\n",
    "    logger.info(f\"Mean IoU for {sub_dir}/{domain}/{stereo_dir}: {miou:.4f}\")\n",
    "    logger.info(f\"Mean DSC for {sub_dir}/{domain}/{stereo_dir}: {mdsc:.4f}\")\n",
    "\n",
    "    results_file = results_dir + f\"/{model}\" + f\"/{split}.json\"\n",
    "    if os.path.exists(results_file):\n",
    "        print(f\"Loading existing results file: {results_file}\")\n",
    "        with open(results_file, \"r\") as f:\n",
    "            all_results = json.load(f)\n",
    "    else:\n",
    "        print(f\"Creating new results file: {results_file}\")\n",
    "        all_results = {}\n",
    "    \n",
    "    all_results[frames_path] = {\n",
    "        \"miou\": miou,\n",
    "        \"mdsc\": mdsc\n",
    "    }\n",
    "\n",
    "    with open(results_file, \"w\") as f:\n",
    "        json.dump(all_results, f)\n",
    "    print(f\"Results saved to {results_file}\")\n",
    "\n",
    "    overall_end = time.time()\n",
    "    total_time = overall_end - overall_start\n",
    "    print(f\"Processing video took {total_time:.2f} seconds.\")\n",
    "    logger.info(f\"Results for {sub_dir}/{domain}/{stereo_dir} saved.\")\n",
    "    logger.info(f\"Processing video took {total_time:.2f} seconds.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return miou, mdsc\n",
    "\n",
    "def process_split(sub_dirs, domains, split):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Running inference for split: {split}\")\n",
    "    logger.info(f\"Using Model: {model}\")\n",
    "    logger.info(f\"Annotation mode: {'manual' if is_manual_annotation else 'auto'}\")\n",
    "    logger.info(f\"Performing tta: {'yes' if perform_tta else 'no'}\")\n",
    "    if not is_manual_annotation:\n",
    "        logger.info(f\"Number of positive point annotations per tool: {num_pos_points_per_tool}\")\n",
    "        logger.info(f\"Number of negative point annotations per tool: {num_neg_points_per_tool}\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    logger.info(f\"----------------Running inference for split {split}-------------\")\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    print(f\"Processing {len(sub_dirs)} sub-directories and {len(domains)} domains\")\n",
    "    logger.info(f\"Processing {len(sub_dirs)} sub-directories and {len(domains)} domains for split {split}\")\n",
    "        \n",
    "    sub_dir_results = {}\n",
    "    for sub_dir in tqdm(sub_dirs, desc=f\"Processing sub-directories\"):\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(f\"Processing sub-directory: {sub_dir}\")\n",
    "        logger.info(f\"Processing sub-directory: {sub_dir}\")\n",
    "        domain_results = {}\n",
    "        for domain in tqdm(domains, desc=f\"Processing domains\"):\n",
    "            print(f\"\\nProcessing domain: {domain}\")\n",
    "            logger.info(f\"Processing domain: {domain} in {sub_dir}\")\n",
    "            left_video_frames_path = sub_dir + \"/\" + domain + \"/left\"\n",
    "            right_video_frames_path = sub_dir + \"/\" + domain + \"/right\"\n",
    "\n",
    "            print(f\"Processing left camera video...\")\n",
    "            left_miou, left_msdc = process_video(left_video_frames_path, sub_dir, domain, split, True)\n",
    "            \n",
    "            print(f\"Processing right camera video...\")\n",
    "            right_miou, right_msdc = process_video(right_video_frames_path, sub_dir, domain, split, False)\n",
    "\n",
    "            overall_miou = (left_miou + right_miou) / 2\n",
    "            overall_msdc = (left_msdc + right_msdc) / 2\n",
    "\n",
    "            print(f\"\\nResults for {sub_dir}/{domain}:\")\n",
    "            print(f\"  Left: IoU={left_miou:.4f}, DSC={left_msdc:.4f}\")\n",
    "            print(f\"  Right: IoU={right_miou:.4f}, DSC={right_msdc:.4f}\")\n",
    "            print(f\"  Overall: IoU={overall_miou:.4f}, DSC={overall_msdc:.4f}\")\n",
    "            \n",
    "            logger.info(f\"Results for {sub_dir}/{domain}: Left IoU={left_miou:.4f}, Right IoU={right_miou:.4f}, Overall IoU={overall_miou:.4f}\")\n",
    "\n",
    "            domain_results[domain] = {\n",
    "                \"left_miou\": left_miou,\n",
    "                \"left_msdc\": left_msdc,\n",
    "                \"right_miou\": right_miou,\n",
    "                \"right_msdc\": right_msdc,\n",
    "                \"overall_miou\": overall_miou,\n",
    "                \"overall_msdc\": overall_msdc\n",
    "            }\n",
    "\n",
    "        sub_dir_results[sub_dir] = domain_results\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"SUMMARY RESULTS FOR SPLIT: {split}\")\n",
    "    print(\"=\"*60)\n",
    "    logger.info(f\"SUMMARY RESULTS FOR SPLIT: {split}\")\n",
    "    logger.info(f\"Using Model: {model}\")\n",
    "    logger.info(f\"Annotation mode: {'manual' if is_manual_annotation else 'auto'}\")\n",
    "    logger.info(f\"Performing tta: {'yes' if perform_tta else 'no'}\")\n",
    "    if not is_manual_annotation:\n",
    "        logger.info(f\"Number of positive point annotations per tool: {num_pos_points_per_tool}\")\n",
    "        logger.info(f\"Number of negative point annotations per tool: {num_neg_points_per_tool}\")\n",
    "\n",
    "    # Domain-wise results\n",
    "    print(\"\\nDomain-wise Results:\")\n",
    "    logger.info(\"Domain-wise Results:\")\n",
    "    domain_results_data = {}\n",
    "    for domain in domains:\n",
    "        left_mious = [sub_dir_results[sub_dir][domain][\"left_miou\"] for sub_dir in sub_dirs]\n",
    "        right_mious = [sub_dir_results[sub_dir][domain][\"right_miou\"] for sub_dir in sub_dirs]\n",
    "        overall_mious = [sub_dir_results[sub_dir][domain][\"overall_miou\"] for sub_dir in sub_dirs]\n",
    "\n",
    "        left_msdcs = [sub_dir_results[sub_dir][domain][\"left_msdc\"] for sub_dir in sub_dirs]\n",
    "        right_msdcs = [sub_dir_results[sub_dir][domain][\"right_msdc\"] for sub_dir in sub_dirs]\n",
    "        overall_msdcs = [sub_dir_results[sub_dir][domain][\"overall_msdc\"] for sub_dir in sub_dirs]\n",
    "\n",
    "        print(f\"\\nDomain: {domain}\")\n",
    "        print(f\"  Left Frame IoU: {np.mean(left_mious):.4f}\")\n",
    "        print(f\"  Right Frame IoU: {np.mean(right_mious):.4f}\")\n",
    "        print(f\"  Overall IoU: {np.mean(overall_mious):.4f}\")\n",
    "        print(f\"  Left Frame DSC: {np.mean(left_msdcs):.4f}\")\n",
    "        print(f\"  Right Frame DSC: {np.mean(right_msdcs):.4f}\")\n",
    "        print(f\"  Overall DSC: {np.mean(overall_msdcs):.4f}\")\n",
    "        \n",
    "        logger.info(f\"Domain {domain} - Left IoU: {np.mean(left_mious):.4f}, Right IoU: {np.mean(right_mious):.4f}, Overall IoU: {np.mean(overall_mious):.4f}\")\n",
    "        logger.info(f\"Domain {domain} - Left DSC: {np.mean(left_msdcs):.4f}, Right DSC: {np.mean(right_msdcs):.4f}, Overall DSC: {np.mean(overall_msdcs):.4f}\")\n",
    "        \n",
    "        domain_results_data[domain] = {\n",
    "            \"left_miou\": np.mean(left_mious),\n",
    "            \"right_miou\": np.mean(right_mious),\n",
    "            \"overall_miou\": np.mean(overall_mious),\n",
    "            \"left_mdsc\": np.mean(left_msdcs),\n",
    "            \"right_mdsc\": np.mean(right_msdcs),\n",
    "            \"overall_mdsc\": np.mean(overall_msdcs)\n",
    "        }\n",
    "\n",
    "    # Overall results across all domains and sub-dirs\n",
    "    left_mious = [np.mean([sub_dir_results[sub_dir][domain][\"left_miou\"] for domain in domains]) for sub_dir in sub_dirs]\n",
    "    right_mious = [np.mean([sub_dir_results[sub_dir][domain][\"right_miou\"] for domain in domains]) for sub_dir in sub_dirs]\n",
    "    overall_mious = [np.mean([sub_dir_results[sub_dir][domain][\"overall_miou\"] for domain in domains]) for sub_dir in sub_dirs]\n",
    "\n",
    "    left_msdcs = [np.mean([sub_dir_results[sub_dir][domain][\"left_msdc\"] for domain in domains]) for sub_dir in sub_dirs]\n",
    "    right_msdcs = [np.mean([sub_dir_results[sub_dir][domain][\"right_msdc\"] for domain in domains]) for sub_dir in sub_dirs]\n",
    "    overall_msdcs = [np.mean([sub_dir_results[sub_dir][domain][\"overall_msdc\"] for domain in domains]) for sub_dir in sub_dirs]\n",
    "\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"FINAL RESULTS ACROSS ALL DOMAINS AND SUB-DIRECTORIES:\")\n",
    "    print(f\"  Left Frame IoU: {np.mean(left_mious):.4f}\")\n",
    "    print(f\"  Right Frame IoU: {np.mean(right_mious):.4f}\")\n",
    "    print(f\"  Overall IoU: {np.mean(overall_mious):.4f}\")\n",
    "    print(f\"  Left Frame DSC: {np.mean(left_msdcs):.4f}\")\n",
    "    print(f\"  Right Frame DSC: {np.mean(right_msdcs):.4f}\")\n",
    "    print(f\"  Overall DSC: {np.mean(overall_msdcs):.4f}\")\n",
    "\n",
    "    logger.info(\"FINAL RESULTS ACROSS ALL DOMAINS AND SUB-DIRECTORIES:\")\n",
    "    logger.info(f\"Left Frame IoU: {np.mean(left_mious):.4f}\")\n",
    "    logger.info(f\"Right Frame IoU: {np.mean(right_mious):.4f}\")\n",
    "    logger.info(f\"Overall IoU: {np.mean(overall_mious):.4f}\")\n",
    "    logger.info(f\"Left Frame DSC: {np.mean(left_msdcs):.4f}\")\n",
    "    logger.info(f\"Right Frame DSC: {np.mean(right_msdcs):.4f}\")\n",
    "    logger.info(f\"Overall DSC: {np.mean(overall_msdcs):.4f}\")\n",
    "\n",
    "    overall_end = time.time()\n",
    "    total_time = overall_end - overall_start\n",
    "    print(f\"\\nTotal time taken for split {split}: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    logger.info(f\"Total time taken for split {split}: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    \n",
    "    csv_file = f'data/results/results.csv'\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    data = {\n",
    "        'timestamp': timestamp,\n",
    "        'model': model,\n",
    "        'split': split,\n",
    "        'annotation_mode': 'manual' if is_manual_annotation else 'auto',\n",
    "        'num_pos_points': num_pos_points_per_tool if not is_manual_annotation else 'N/A',\n",
    "        'num_neg_points': num_neg_points_per_tool if not is_manual_annotation else 'N/A',\n",
    "        'tta': 'yes' if perform_tta else 'no',\n",
    "        'overall_left_miou': np.mean(left_mious),\n",
    "        'overall_right_miou': np.mean(right_mious),\n",
    "        'overall_miou': np.mean(overall_mious),\n",
    "        'overall_left_mdsc': np.mean(left_msdcs),\n",
    "        'overall_right_mdsc': np.mean(right_msdcs),\n",
    "        'overall_mdsc': np.mean(overall_msdcs),\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }\n",
    "    \n",
    "    # Add domain-specific results\n",
    "    for domain in domains:\n",
    "        data[f'{domain}_left_miou'] = domain_results_data[domain]['left_miou']\n",
    "        data[f'{domain}_right_miou'] = domain_results_data[domain]['right_miou']\n",
    "        data[f'{domain}_overall_miou'] = domain_results_data[domain]['overall_miou']\n",
    "        data[f'{domain}_left_mdsc'] = domain_results_data[domain]['left_mdsc']\n",
    "        data[f'{domain}_right_mdsc'] = domain_results_data[domain]['right_mdsc']\n",
    "        data[f'{domain}_overall_mdsc'] = domain_results_data[domain]['overall_mdsc']\n",
    "    \n",
    "    # Convert to DataFrame for a single row\n",
    "    df_new = pd.DataFrame([data])\n",
    "    \n",
    "    # Check if file exists and append, or create new\n",
    "    if os.path.exists(csv_file):\n",
    "        df_existing = pd.read_csv(csv_file)\n",
    "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(csv_file), exist_ok=True)\n",
    "        df_new.to_csv(csv_file, index=False)\n",
    "    \n",
    "    print(f\"Results saved to CSV file: {csv_file}\")\n",
    "    logger.info(f\"Results saved to CSV file: {csv_file}\")\n",
    "\n",
    "\n",
    "process_split(sub_dirs, domains, \"val\")\n",
    "# process_split(test_sub_dirs, test_domains, \"test\")\n",
    "# process_split(train_sub_dirs, train_domains, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domains = ['bg_change', 'blood', 'low_brightness', 'regular', 'smoke']\n",
    "# annotations = None\n",
    "# with open('data/annotations/auto/val.json', 'r') as f:\n",
    "#     annotations = json.load(f)\n",
    "\n",
    "# path = \"data/masks/sam2.1_hiera_base_plus/val/data-raw-SegSTRONGC_val-val-1-2-bg_change-right.pkl\"\n",
    "# with open(path, 'rb') as f:\n",
    "#     mass = pickle.load(f)\n",
    "#     for video_path, video_data in mass.items():\n",
    "#         # print(video_path, video_data)\n",
    "#         for frame_id, frame_data in video_data.items():\n",
    "#             overall_mask = np.zeros((1080, 1920), dtype=bool)\n",
    "#             for data in frame_data:\n",
    "#                 for object_id, mask in data.items():\n",
    "#                     overall_mask = np.logical_or(overall_mask, mask[0])\n",
    "\n",
    "#             ground_truth_masks_path = video_path\n",
    "#             for domain in domains:\n",
    "#                 if domain in video_path:\n",
    "#                     ground_truth_masks_path = video_path.replace(domain, 'ground_truth')\n",
    "#                     break\n",
    "#             ground_truth_masks_path = ground_truth_masks_path + \"/\" + str(frame_id) + \".jpg\"\n",
    "#             ground_truth_mask = cv2.imread(ground_truth_masks_path, cv2.IMREAD_GRAYSCALE)\n",
    "#             ground_truth_mask = (ground_truth_mask > 0).astype(np.bool_)\n",
    "\n",
    "#             original_image = cv2.imread(video_path + \"/\" + str(frame_id) + \".jpg\")\n",
    "#             original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#             # frame_annotations = annotations[video_path.replace(\"/left\", \"\").replace(\"/right\", \"\")]\n",
    "#             frame_annotations = annotations[video_path]\n",
    "\n",
    "#             # place dots in the original image for each annotation\n",
    "#             for object_id, object_annotations in frame_annotations.items():\n",
    "#                 # print(object_annotations)\n",
    "#                 for annotation in object_annotations:\n",
    "#                     x = annotation['x']\n",
    "#                     y = annotation['y']\n",
    "#                     label = annotation['label']\n",
    "#                     if object_id == \"0\":\n",
    "#                         original_image = cv2.circle(original_image, (x, y), 10, (0, 255, 0), -1)\n",
    "#                     else:\n",
    "#                         original_image = cv2.circle(original_image, (x, y), 10, (255, 0, 0), -1)\n",
    "\n",
    "#             #show the masks and the original image\n",
    "#             fig, axs = plt.subplots(1, 3, figsize=(30, 15))\n",
    "#             axs[0].imshow(original_image)\n",
    "#             axs[0].set_title(\"Original Image\")\n",
    "#             axs[1].imshow(overall_mask)\n",
    "#             axs[1].set_title(\"Overall Mask\")\n",
    "#             axs[2].imshow(ground_truth_mask)\n",
    "#             axs[2].set_title(\"Ground Truth Mask\")\n",
    "#             plt.show()\n",
    "#             break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
