{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2 \n",
    "from ultralytics import YOLO\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import gc\n",
    "import shutil\n",
    "import requests\n",
    "from torchvision.ops import box_convert, nms\n",
    "\n",
    "from SAM2.sam2.sam2.build_sam import build_sam2_video_predictor\n",
    "from groundingdino.util.inference import load_model, load_image, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 63\n"
     ]
    }
   ],
   "source": [
    "# !sudo mount -t drvfs E: /mnt/g\n",
    "# mogrify -format jpg *.png && rm *.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DESCRIPTION = \"sam2 with grounding dino box prompts. only first frame boxes are used. no tta. boxes were precomputed by using nms and other shit\"\n",
    "SPLITS_TO_RUN = [\"test\", \"val\", \"train\"]\n",
    "TOTAL_FRAMES_PER_VIDEO = 300\n",
    "\n",
    "# dataset paths\n",
    "DATASET_ROOT_PATH = \"data/raw\"\n",
    "VAL_PATH = DATASET_ROOT_PATH + \"/SegSTRONGC_val/val\"\n",
    "TEST_PATH = DATASET_ROOT_PATH + \"/SegSTRONGC_test/test\"\n",
    "TRIAN_PATH = DATASET_ROOT_PATH + \"/SegSTRONGC_train/train\"\n",
    "USE_GOKUL_SPLIT = False\n",
    "\n",
    "VAL_FOLDERS = {\"1\": [\"0\", \"1\", \"2\"]}\n",
    "TEST_FOLDERS = {\"9\": [\"0\", \"1\", \"2\"]} \n",
    "TRAIN_FOLDERS = {\"3\": [\"0\", \"2\"], \"4\": [\"0\", \"1\", \"2\"], \"5\": [\"0\", \"2\"], \"7\": [\"0\", \"1\"], \"8\": [\"1\", \"2\"]}\n",
    "\n",
    "GOKUL_VAL_FOLDERS = {\"9\": [\"0\", \"1\"]}\n",
    "GOKUL_TEST_FOLDERS = {\"9\": [\"2\"]}\n",
    "GOKUL_TRAIN_FOLDERS = {\"1\": [\"0\", \"1\", \"2\"]}\n",
    "\n",
    "# domains (ground truth is 'ground_truth')\n",
    "VAL_DOMAINS = ['bg_change', 'blood', 'low_brightness', 'regular', 'smoke']\n",
    "TEST_DOMAINS = ['bg_change', 'blood', 'low_brightness', 'regular', 'smoke']\n",
    "TRAIN_DOMAINS = ['regular']\n",
    "\n",
    "# prompts\n",
    "SHOULD_USE_MANUAL_PROMPT = False\n",
    "SHOULD_USE_BOX_PROMPT = False\n",
    "SHOULD_SAMPLE_GROUND_TRUTH = True\n",
    "SHOULD_VISUALIZE_PROMPTS = False\n",
    "REFRESH_PROMPTS = True\n",
    "NUM_POS_POINTS_PER_TOOL = 1\n",
    "NUM_NEG_POINTS_PER_TOOL = 0\n",
    "PROMPTS_ROOT_PATH = \"data/prompts\"\n",
    "PROMPTING_STRATEGIES = [\"first\", \"all\", \"dynamic\"]\n",
    "PROMPTING_STRATEGY = PROMPTING_STRATEGIES[0]\n",
    "DYNAMIC_FRAME_SKIP = 15\n",
    "MODEL_PROMPT_CAPTION = \"tool\"\n",
    "MODEL_PROMPT_BOX_THRESHOLD = 0.2\n",
    "MODEL_PROMPT_TEXT_THRESHOLD = 0.25\n",
    "MODEL_PROMPT_NMS_THRESHOLD = 0.3\n",
    "MODEL_PROMPT_AREA_THRESHOLD = 0.9\n",
    "\n",
    "# test time adaptation\n",
    "SHOULD_PERFROM_CYCLIC_TTA = False\n",
    "\n",
    "# results\n",
    "SAVE_RUN_MASK_LOGITS = False\n",
    "SAVE_IMAGES_ONCE = False\n",
    "SAVE_OUTPUT_MASKS = True\n",
    "BASE_RESULTS_DIR = \"data/results\"\n",
    "MASKS_DIR = BASE_RESULTS_DIR + \"/masks\"\n",
    "\n",
    "# models\n",
    "MODELS = [\"sam2.1_hiera_base_plus\", \"yolo11x-seg\", \"groundingdino-swinb\"]\n",
    "INFERENCE_MODEL = MODELS[0]\n",
    "PROMPT_MODEL = MODELS[2]\n",
    "CHECKPOINTS = {\n",
    "    \"sam2.1_hiera_base_plus\": \"checkpoints/sam2.1_hiera_base_plus.pt\",\n",
    "    \"yolo11x-seg\": \"checkpoints/yolo11x-seg.pt\",\n",
    "    \"groundingdino-swinb\": \"GroundingDINO/weights/groundingdino_swinb_cogcoor.pth\"\n",
    "}\n",
    "MODEL_CONFIGS = {\n",
    "    \"sam2.1_hiera_base_plus\": \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "    \"yolo11x-seg\": None,\n",
    "    \"groundingdino-swinb\": \"GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py\"\n",
    "}\n",
    "\n",
    "# logs\n",
    "LOG_DIR =  \"logs\"\n",
    "DISCORD_WEBHOOK_URL = \"https://discord.com/api/webhooks/1360663640046571580/mNocZ3tLWiUVaMQTnOWqVRJU-HdI9onQuw0Wcr1xn8ZxRdvY51kuf9IcZ2qxRIBE21-x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(path, domain, is_left, num_images=300):\n",
    "    stereo_dir = \"left\" if is_left else \"right\"\n",
    "    image_paths = []\n",
    "\n",
    "    for i in range(num_images):\n",
    "        image_paths.append(path + \"/\" + domain + \"/\" + stereo_dir + \"/\" + str(i) + \".png\")\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "def get_video_paths(base_path, folders, video_folders):\n",
    "    return [f\"{base_path}/{folder}/{video}\" \n",
    "            for folder in folders \n",
    "            for video in video_folders[folder]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_video_folders_path ['data/raw/SegSTRONGC_val/val/1/0', 'data/raw/SegSTRONGC_val/val/1/1', 'data/raw/SegSTRONGC_val/val/1/2']\n",
      "test_video_folders_path ['data/raw/SegSTRONGC_test/test/9/0', 'data/raw/SegSTRONGC_test/test/9/1', 'data/raw/SegSTRONGC_test/test/9/2']\n",
      "train_video_folders_path ['data/raw/SegSTRONGC_train/train/3/0', 'data/raw/SegSTRONGC_train/train/3/2', 'data/raw/SegSTRONGC_train/train/4/0', 'data/raw/SegSTRONGC_train/train/4/1', 'data/raw/SegSTRONGC_train/train/4/2', 'data/raw/SegSTRONGC_train/train/5/0', 'data/raw/SegSTRONGC_train/train/5/2', 'data/raw/SegSTRONGC_train/train/7/0', 'data/raw/SegSTRONGC_train/train/7/1', 'data/raw/SegSTRONGC_train/train/8/1', 'data/raw/SegSTRONGC_train/train/8/2']\n",
      "inference_model_name sam2.1_hiera_base_plus\n",
      "prompts_folder data/prompts/auto/point/1-pos_0-neg\n"
     ]
    }
   ],
   "source": [
    "if not USE_GOKUL_SPLIT:\n",
    "    val_video_folders_path = get_video_paths(VAL_PATH, VAL_FOLDERS.keys(), VAL_FOLDERS)\n",
    "    test_video_folders_path = get_video_paths(TEST_PATH, TEST_FOLDERS.keys(), TEST_FOLDERS)\n",
    "    train_video_folders_path = get_video_paths(TRIAN_PATH, TRAIN_FOLDERS.keys(), TRAIN_FOLDERS)\n",
    "else:\n",
    "    val_video_folders_path = get_video_paths(TEST_PATH, GOKUL_VAL_FOLDERS.keys(), GOKUL_VAL_FOLDERS)  \n",
    "    test_video_folders_path = get_video_paths(TEST_PATH, GOKUL_TEST_FOLDERS.keys(), GOKUL_TEST_FOLDERS)\n",
    "    train_video_folders_path = get_video_paths(VAL_PATH, GOKUL_TRAIN_FOLDERS.keys(), GOKUL_TRAIN_FOLDERS)\n",
    "\n",
    "if \"val\" not in SPLITS_TO_RUN:\n",
    "    val_video_folders_path = []\n",
    "if \"test\" not in SPLITS_TO_RUN:\n",
    "    test_video_folders_path = []\n",
    "if \"train\" not in SPLITS_TO_RUN:\n",
    "    train_video_folders_path = []\n",
    "\n",
    "inference_model_checkpoint = CHECKPOINTS[INFERENCE_MODEL]\n",
    "inference_model_config = MODEL_CONFIGS[INFERENCE_MODEL]\n",
    "\n",
    "prompt_model_checkpoint = CHECKPOINTS[PROMPT_MODEL]\n",
    "prompt_model_config = MODEL_CONFIGS[PROMPT_MODEL]\n",
    "\n",
    "logging.basicConfig(filename=LOG_DIR + f'/{INFERENCE_MODEL}.log', level=logging.INFO, format='%(asctime)s - %(message)s', filemode='a')\n",
    "LOGGER = logging.getLogger()\n",
    "\n",
    "PROMPTS_FOLDER = PROMPTS_ROOT_PATH\n",
    "PROMPTS_FOLDER += \"/manual\" if SHOULD_USE_MANUAL_PROMPT else \"/auto\"\n",
    "PROMPTS_FOLDER += \"/box\" if SHOULD_USE_BOX_PROMPT else \"/point\"\n",
    "if SHOULD_USE_BOX_PROMPT:\n",
    "    PROMPTS_FOLDER += \"/ground_truth\" if SHOULD_SAMPLE_GROUND_TRUTH else \"/groundingdino\"\n",
    "else:\n",
    "    PROMPTS_FOLDER += f\"/{NUM_POS_POINTS_PER_TOOL}-pos_{NUM_NEG_POINTS_PER_TOOL}-neg\"\n",
    "\n",
    "if not SHOULD_USE_MANUAL_PROMPT and SHOULD_USE_BOX_PROMPT and not SHOULD_SAMPLE_GROUND_TRUTH:\n",
    "    prompt_model = load_model(prompt_model_config, prompt_model_checkpoint)\n",
    "\n",
    "print(\"val_video_folders_path\", val_video_folders_path)\n",
    "print(\"test_video_folders_path\", test_video_folders_path)\n",
    "print(\"train_video_folders_path\", train_video_folders_path)\n",
    "print(\"inference_model_name\", INFERENCE_MODEL)\n",
    "print(\"prompts_folder\", PROMPTS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(TP, FP, FN):\n",
    "    return TP / (TP + FP + FN)\n",
    "\n",
    "def calculate_dsc(TP, FP, FN):\n",
    "    return 2 * TP / (2 * TP + FP + FN)\n",
    "\n",
    "def calculate_miou(pred_masks, gt_masks):\n",
    "    ious = []\n",
    "    for i in range(len(pred_masks)):\n",
    "        TP = np.logical_and(pred_masks[i], gt_masks[i])\n",
    "        FP = np.logical_and(pred_masks[i], np.logical_not(gt_masks[i]))\n",
    "        FN = np.logical_and(np.logical_not(pred_masks[i]), gt_masks[i])\n",
    "\n",
    "        iou = calculate_iou(np.sum(TP), np.sum(FP), np.sum(FN))\n",
    "        ious.append(iou)\n",
    "    \n",
    "    return np.mean(ious)\n",
    "\n",
    "def calculate_mdsc(pred_masks, gt_masks):\n",
    "    dscs = []\n",
    "    for i in range(len(pred_masks)):\n",
    "        TP = np.logical_and(pred_masks[i], gt_masks[i])\n",
    "        FP = np.logical_and(pred_masks[i], np.logical_not(gt_masks[i]))\n",
    "        FN = np.logical_and(np.logical_not(pred_masks[i]), gt_masks[i])\n",
    "\n",
    "        dsc = calculate_dsc(np.sum(TP), np.sum(FP), np.sum(FN))\n",
    "        dscs.append(dsc)\n",
    "    \n",
    "    return np.mean(dscs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_annotate(frame_path):\n",
    "    annotations = {0: []}\n",
    "    current_tool = 0\n",
    "    is_positive = True\n",
    "\n",
    "    window_name = \"Manual Annotation of Frame -\" + str(frame_path)\n",
    "    cv2.namedWindow(window_name)\n",
    "\n",
    "    def handle_mouse_click(event, x, y, flags, params):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            if annotations[current_tool][\"points\"] is None:\n",
    "                annotations[current_tool][\"points\"] = []\n",
    "\n",
    "            if annotations[current_tool][\"labels\"] is None:\n",
    "                annotations[current_tool][\"labels\"] = []\n",
    "            \n",
    "            annotations[current_tool][\"points\"].append([x, y])\n",
    "            annotations[current_tool][\"labels\"].append(1 if is_positive else 0)\n",
    "\n",
    "            if is_positive:\n",
    "                cv2.circle(frame, (x, y), 10, (0, 255, 0), -1)\n",
    "            else:\n",
    "                cv2.circle(frame, (x, y), 10, (0, 0, 255), -1)\n",
    "\n",
    "        cv2.imshow(window_name, frame)\n",
    "\n",
    "    cv2.setMouseCallback(window_name, handle_mouse_click)\n",
    "    frame = cv2.imread(frame_path)\n",
    "    original_frame = frame.copy()\n",
    "\n",
    "    while True:\n",
    "        frame = original_frame.copy()\n",
    "        for tool in annotations:\n",
    "            if annotations[tool][\"points\"] is not None:\n",
    "                for point, label in zip(annotations[tool][\"points\"], annotations[tool][\"labels\"]):\n",
    "                    if label == 1:\n",
    "                        cv2.circle(frame, (point[0], point[1]), 10, (0, 255, 0), -1)\n",
    "                    else:\n",
    "                        cv2.circle(frame, (point[0], point[1]), 10, (0, 0, 255), -1)\n",
    "                \n",
    "        display_text = f\"Tool: {current_tool}, Mode: {'Positive' if is_positive else 'Negative'}\"\n",
    "        cv2.putText(frame, display_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(window_name, frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord(\"n\"):\n",
    "            current_tool += 1\n",
    "            is_positive = True\n",
    "            annotations[current_tool] = []\n",
    "        elif key == ord(\"p\"):\n",
    "            is_positive = not is_positive\n",
    "        elif key == ord(\"c\"):\n",
    "            annotations[current_tool] = []\n",
    "        elif key == ord(\"s\"):\n",
    "            break\n",
    "\n",
    "    cv2.destroyWindow(window_name)\n",
    "    return annotations\n",
    "\n",
    "def auto_box_ground_truth_annotate(frame_path):\n",
    "    ground_truth_mask_path = None\n",
    "    annotations = {}\n",
    "    # annotations[current_tool].append({\n",
    "    #             \"x\": x,\n",
    "    #             \"y\": y,\n",
    "    #             \"label\": 1 if is_positive else 0\n",
    "    #         })\n",
    "\n",
    "    for domain in VAL_DOMAINS:\n",
    "        if domain in frame_path:\n",
    "            ground_truth_mask_path = frame_path.replace(domain, \"ground_truth\")\n",
    "            break\n",
    "    \n",
    "    if ground_truth_mask_path is None:\n",
    "        raise ValueError(\"Ground truth path not found.\")\n",
    "    else:\n",
    "        ground_truth_mask = cv2.imread(ground_truth_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        ground_truth_mask = (ground_truth_mask > 0).astype(np.bool_)\n",
    "        _, labels = cv2.connectedComponents(ground_truth_mask.astype(np.uint8))\n",
    "\n",
    "        # get unique labels, and get count for each label, then sort by count in descending order\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        sorted_labels = unique_labels[np.argsort(-counts)]\n",
    "\n",
    "        background_label = sorted_labels[0]\n",
    "        first_tool_label = sorted_labels[1]\n",
    "\n",
    "        if len(sorted_labels) > 2:\n",
    "            second_tool_label = sorted_labels[2]\n",
    "        else:\n",
    "            second_tool_label = -1\n",
    "\n",
    "        #for each tool, get the centroid, and add num_auto_points - 1 sampled random points\n",
    "        annotations[0] = {}\n",
    "        annotations[1] = {}\n",
    "        boxes = []\n",
    "\n",
    "        for label, obj_id in [(first_tool_label, 0), (second_tool_label, 1)]:\n",
    "\n",
    "            if label == -1:\n",
    "                continue\n",
    "\n",
    "            label_indices = np.where(labels == label)\n",
    "            all_points = []\n",
    "\n",
    "            for i in range(len(label_indices[0])):\n",
    "                x = int(label_indices[1][i])\n",
    "                y = int(label_indices[0][i])\n",
    "                all_points.append((x, y))\n",
    "\n",
    "            rightmost = max(all_points, key=lambda x: x[0])[0]\n",
    "            leftmost = min(all_points, key=lambda x: x[0])[0]\n",
    "            topmost = min(all_points, key=lambda x: x[1])[1]\n",
    "            bottommost = max(all_points, key=lambda x: x[1])[1]\n",
    "\n",
    "            x1 = leftmost\n",
    "            y1 = topmost\n",
    "            x2 = rightmost\n",
    "            y2 = bottommost\n",
    "\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "        boxes = sorted(boxes, key=lambda x: x[0])\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            x1 = box[0]\n",
    "            y1 = box[1]\n",
    "            x2 = box[2]\n",
    "            y2 = box[3]\n",
    "\n",
    "            annotations[i][\"bbox\"] = {\n",
    "                    \"box\": [x1, y1, x2, y2],\n",
    "                    \"conf\": 1.0\n",
    "                }\n",
    "\n",
    "        return annotations\n",
    "\n",
    "def auto_box_model_annotate(frame_path):\n",
    "    # print(\"path: \", frame_path)\n",
    "    annotations = {}\n",
    "    img_src, img = load_image(frame_path)\n",
    "\n",
    "    boxes, logits, _ = predict(\n",
    "        model=prompt_model,\n",
    "        image=img,\n",
    "        caption=MODEL_PROMPT_CAPTION,\n",
    "        box_threshold=MODEL_PROMPT_BOX_THRESHOLD,\n",
    "        text_threshold=MODEL_PROMPT_TEXT_THRESHOLD\n",
    "    )\n",
    "\n",
    "    h, w, _ = img_src.shape\n",
    "    all_boxes_xyxy = box_convert(boxes=boxes * torch.Tensor([w, h, w, h]), in_fmt=\"cxcywh\", out_fmt=\"xyxy\")\n",
    "    all_conf = logits\n",
    "    all_indices = np.arange(len(all_boxes_xyxy))\n",
    "\n",
    "    nms_indices = nms(all_boxes_xyxy, all_conf, MODEL_PROMPT_NMS_THRESHOLD).numpy().tolist()\n",
    "\n",
    "    for i in range(len(nms_indices)):\n",
    "        box = all_boxes_xyxy[nms_indices[i]]\n",
    "\n",
    "        x1 = box[0]\n",
    "        y1 = box[1]\n",
    "        x2 = box[2]\n",
    "        y2 = box[3]\n",
    "\n",
    "        area = (x2 - x1) * (y2 - y1)\n",
    "        if area > MODEL_PROMPT_AREA_THRESHOLD * (w * h):\n",
    "            if len(nms_indices) > 1:\n",
    "                nms_indices.pop(i)\n",
    "            break\n",
    "\n",
    "    indices_to_remove = []\n",
    "    for i in range(len(nms_indices)):\n",
    "        for j in range(i + 1, len(nms_indices)):\n",
    "            box1 = all_boxes_xyxy[nms_indices[i]]\n",
    "            box2 = all_boxes_xyxy[nms_indices[j]]\n",
    "\n",
    "            box1_x1 = box1[0]\n",
    "            box1_y1 = box1[1]\n",
    "            box1_x2 = box1[2]\n",
    "            box1_y2 = box1[3]\n",
    "\n",
    "            box2_x1 = box2[0]\n",
    "            box2_y1 = box2[1]\n",
    "            box2_x2 = box2[2]\n",
    "            box2_y2 = box2[3]\n",
    "\n",
    "            area1 = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)\n",
    "            area2 = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)\n",
    "\n",
    "            x_overlap = max(0, min(box1_x2, box2_x2) - max(box1_x1, box2_x1))\n",
    "            y_overlap = max(0, min(box1_y2, box2_y2) - max(box1_y1, box2_y1))\n",
    "            intersection = x_overlap * y_overlap\n",
    "\n",
    "            union = area1 + area2 - intersection\n",
    "\n",
    "            larger_area = max(area1, area2)\n",
    "\n",
    "            if union * MODEL_PROMPT_AREA_THRESHOLD <= larger_area:\n",
    "                if area1 < area2:\n",
    "                    if nms_indices[i] not in indices_to_remove:\n",
    "                        if len(set(nms_indices) - set(indices_to_remove)) > 1:\n",
    "                            indices_to_remove.append(nms_indices[i])\n",
    "                else:\n",
    "                    if nms_indices[j] not in indices_to_remove:\n",
    "                        if len(set(nms_indices) - set(indices_to_remove)) > 1:\n",
    "                            indices_to_remove.append(nms_indices[j])\n",
    "    \n",
    "    nms_indices = list(set(nms_indices) - set(indices_to_remove))\n",
    "    final_indices = nms_indices.copy()\n",
    "\n",
    "    if len(final_indices) > 0:\n",
    "        final_boxes = all_boxes_xyxy[final_indices]\n",
    "        final_conf = all_conf[final_indices]\n",
    "\n",
    "        sort_indices = np.argsort(final_boxes[:, 0])\n",
    "        final_boxes = final_boxes[sort_indices]\n",
    "        final_conf = final_conf[sort_indices]\n",
    "\n",
    "        for i in range(min(len(final_boxes), 2)):\n",
    "            x1, y1, x2, y2 = map(int, final_boxes[i])\n",
    "            conf = final_conf[i]\n",
    "\n",
    "            if i not in annotations:\n",
    "                annotations[i] = {}\n",
    "\n",
    "            annotations[i][\"bbox\"] = {\n",
    "                \"box\": [x1, y1, x2, y2],\n",
    "                \"conf\": float(conf)\n",
    "            }\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def auto_point_annotate(frame_path):\n",
    "    ground_truth_mask_path = None\n",
    "    annotations = {}\n",
    "    # annotations[current_tool].append({\n",
    "    #             \"x\": x,\n",
    "    #             \"y\": y,\n",
    "    #             \"label\": 1 if is_positive else 0\n",
    "    #         })\n",
    "\n",
    "    for domain in VAL_DOMAINS:\n",
    "        if domain in frame_path:\n",
    "            ground_truth_mask_path = frame_path.replace(domain, \"ground_truth\")\n",
    "            break\n",
    "    \n",
    "    if ground_truth_mask_path is None:\n",
    "        raise ValueError(\"Ground truth path not found.\")\n",
    "    else:\n",
    "        ground_truth_mask = cv2.imread(ground_truth_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        ground_truth_mask = (ground_truth_mask > 0).astype(np.bool_)\n",
    "        _, labels = cv2.connectedComponents(ground_truth_mask.astype(np.uint8))\n",
    "\n",
    "        # get unique labels, and get count for each label, then sort by count in descending order\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        sorted_labels = unique_labels[np.argsort(-counts)]\n",
    "\n",
    "        background_label = sorted_labels[0]\n",
    "        first_tool_label = sorted_labels[1]\n",
    "\n",
    "        if len(sorted_labels) > 2:\n",
    "            second_tool_label = sorted_labels[2]\n",
    "        else:\n",
    "            second_tool_label = -1\n",
    "\n",
    "        #for each tool, get the centroid, and add num_auto_points - 1 sampled random points\n",
    "        annotations[0] = {}\n",
    "        annotations[1] = {}\n",
    "\n",
    "        for label, obj_id in [(first_tool_label, 0), (second_tool_label, 1)]:\n",
    "\n",
    "            if label == -1:\n",
    "                continue\n",
    "            \n",
    "            mask = labels == label\n",
    "            mask = mask.astype(np.uint8)\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            contour = contours[0]\n",
    "            M = cv2.moments(contour)\n",
    "            cx = int(M['m10'] / M['m00'])\n",
    "            cy = int(M['m01'] / M['m00'])\n",
    "\n",
    "            if \"points\" not in annotations[obj_id]:\n",
    "                annotations[obj_id][\"points\"] = []\n",
    "\n",
    "            if \"labels\" not in annotations[obj_id]:\n",
    "                annotations[obj_id][\"labels\"] = []\n",
    "\n",
    "            if NUM_POS_POINTS_PER_TOOL > 0:\n",
    "                annotations[obj_id][\"points\"].append([cx, cy])\n",
    "                annotations[obj_id][\"labels\"].append(1)\n",
    "\n",
    "            label_indices = np.where(labels == label)\n",
    "            if NUM_POS_POINTS_PER_TOOL > 1:\n",
    "                random_indices = np.random.choice(len(label_indices[0]), NUM_POS_POINTS_PER_TOOL - 1, replace=False)\n",
    "            else:\n",
    "                random_indices = []\n",
    "            for i in random_indices:\n",
    "                x = int(label_indices[1][i])\n",
    "                y = int(label_indices[0][i])\n",
    "                if ground_truth_mask[y, x]:\n",
    "                    annotations[obj_id][\"points\"].append([x, y])\n",
    "                    annotations[obj_id][\"labels\"].append(1)\n",
    "\n",
    "            label_indices = np.where(labels == background_label)\n",
    "            # print(\"HIII \", label_indices)\n",
    "            random_indices = np.random.choice(len(label_indices[0]), NUM_NEG_POINTS_PER_TOOL, replace=False)\n",
    "            for i in random_indices:\n",
    "                x = int(label_indices[1][i])\n",
    "                y = int(label_indices[0][i])\n",
    "                if not ground_truth_mask[y, x]:\n",
    "                    annotations[obj_id][\"points\"].append([x, y])\n",
    "                    annotations[obj_id][\"labels\"].append(0)\n",
    "\n",
    "    return annotations\n",
    "def annotate_frames(sub_dirs, domains, split, num_frames=300):\n",
    "    print(\"Annotating split: \", split)\n",
    "\n",
    "    all_annotations = {}\n",
    "\n",
    "    for sub_dir in tqdm(sub_dirs):\n",
    "        for domain in tqdm(domains):\n",
    "            for frame in tqdm(range(num_frames // 2)):\n",
    "                left_video_frames_path = sub_dir + \"/\" + domain + \"/left\"\n",
    "                right_video_frames_path = sub_dir + \"/\" + domain + \"/right\"\n",
    "\n",
    "                first_left_frame = left_video_frames_path + \"/\" + str(frame) + \".png\"\n",
    "                first_right_frame = right_video_frames_path + \"/\" + str(frame) + \".png\"\n",
    "\n",
    "                last_left_frame = left_video_frames_path + \"/\" + str(TOTAL_FRAMES_PER_VIDEO - frame - 1) + \".png\"\n",
    "                last_right_frame = right_video_frames_path + \"/\" + str(TOTAL_FRAMES_PER_VIDEO - frame - 1) + \".png\"\n",
    "\n",
    "                if SHOULD_USE_MANUAL_PROMPT:\n",
    "                    left_annotations = manual_annotate(first_left_frame)\n",
    "                    right_annotations = manual_annotate(first_right_frame)\n",
    "\n",
    "                    left_reverse_annotations = manual_annotate(last_left_frame)\n",
    "                    right_reverse_annotations = manual_annotate(last_right_frame)\n",
    "                else:\n",
    "                    if SHOULD_USE_BOX_PROMPT:\n",
    "                        if SHOULD_SAMPLE_GROUND_TRUTH:\n",
    "                            left_annotations = auto_box_ground_truth_annotate(first_left_frame)\n",
    "                            right_annotations = auto_box_ground_truth_annotate(first_right_frame)\n",
    "\n",
    "                            left_reverse_annotations = auto_box_ground_truth_annotate(last_left_frame)\n",
    "                            right_reverse_annotations = auto_box_ground_truth_annotate(last_right_frame)\n",
    "                        else:\n",
    "                            left_annotations = auto_box_model_annotate(first_left_frame)\n",
    "                            right_annotations = auto_box_model_annotate(first_right_frame)\n",
    "\n",
    "                            left_reverse_annotations = auto_box_model_annotate(last_left_frame)\n",
    "                            right_reverse_annotations = auto_box_model_annotate(last_right_frame)\n",
    "                    else:\n",
    "                        left_annotations = auto_point_annotate(first_left_frame)\n",
    "                        right_annotations = auto_point_annotate(first_right_frame)\n",
    "\n",
    "                        left_reverse_annotations = auto_point_annotate(last_left_frame)\n",
    "                        right_reverse_annotations = auto_point_annotate(last_right_frame)\n",
    "\n",
    "                if sub_dir + \"/\" + domain + \"/left\" not in all_annotations:\n",
    "                    all_annotations[sub_dir + \"/\" + domain + \"/left\"] = {}\n",
    "\n",
    "                if sub_dir + \"/\" + domain + \"/right\" not in all_annotations:\n",
    "                    all_annotations[sub_dir + \"/\" + domain + \"/right\"] = {}\n",
    "\n",
    "                all_annotations[sub_dir + \"/\" + domain + \"/left\"][frame] = left_annotations\n",
    "                all_annotations[sub_dir + \"/\" + domain + \"/right\"][frame] = right_annotations\n",
    "                all_annotations[sub_dir + \"/\" + domain + \"/left\"][TOTAL_FRAMES_PER_VIDEO - frame - 1] = left_reverse_annotations\n",
    "                all_annotations[sub_dir + \"/\" + domain + \"/right\"][TOTAL_FRAMES_PER_VIDEO - frame - 1] = right_reverse_annotations\n",
    "\n",
    "                annotation_file = PROMPTS_FOLDER + f\"/{split}.json\"\n",
    "\n",
    "                os.makedirs(PROMPTS_FOLDER, exist_ok=True)\n",
    "                with open(annotation_file, \"w\") as f:\n",
    "                    json.dump(all_annotations, f)\n",
    "\n",
    "            print(f\"Domain annotated: {domain}\")\n",
    "        print(f\"Subdir annotated: {sub_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotating split:  val\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d496797d9e44c21bfa62f7da1acda85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1edef0f7c4b407bbbb85b5495e25774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6002038f3afa4be38183dbc28933f831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: bg_change\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc06a14e8f0c4951aaca380396938099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: blood\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa610e41ca5467caba9f42b6a8f84b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: low_brightness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3c1c19a75541c0af05b2caea119263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7488c93186e6411bacd1601e7617386e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: smoke\n",
      "Subdir annotated: data/raw/SegSTRONGC_val/val/1/0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca4605d8ca042b2a65990ab8a5e23ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c27f3b0c2cf410dbe414f91304ce886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: bg_change\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2e8b3ad0534a53acc76064fb771545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: blood\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03450a4c39f44a3483fecac365c393b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: low_brightness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf34d4a8eeb46bdb870f0bae98ecc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22951151e2a7475db43d15cfb2a1e2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: smoke\n",
      "Subdir annotated: data/raw/SegSTRONGC_val/val/1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de4a491dd82416c97628f7fd5034260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2820845975c4d2b8522ad637aab1302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: bg_change\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b5264b827e49a3a79eafd8d080ac04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: blood\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d701fc470443cbb49303a4f68d13f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: low_brightness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418cb608eb9f400485f5ce04fa043e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c8d889d6f54bddbacb784b16bf336e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: smoke\n",
      "Subdir annotated: data/raw/SegSTRONGC_val/val/1/2\n",
      "Annotating split:  test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae533e943f2417d92a105ac2100fddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2bdb393251488288fa1118965bb3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653c876ae83e41cb9288e2fe8ef52bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: bg_change\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2271e6b66487498d8d38af87a3c79cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: blood\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef1b34fb96f4bbe8d396dd4efbb421f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: low_brightness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ab8b4dc04c4bdf84daa92c1298c277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c8b488c8a94e07bb6c6f8aca9b1696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: smoke\n",
      "Subdir annotated: data/raw/SegSTRONGC_test/test/9/0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cf91e5b12d41fe9ccef2f6a499cec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94c13c0079344e9a31311028e339be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: bg_change\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86c215d5ac34d0090835498849c206c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: blood\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ec537f73bf45a0bc4f2121bdb23a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: low_brightness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a45f6a24e814290bcc65fa0759240cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b623b23637484ba53e466e924502a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: smoke\n",
      "Subdir annotated: data/raw/SegSTRONGC_test/test/9/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328700f140294542a1b55f9b4e56b231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4928b70bc9414871bb11991291ea9772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: bg_change\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad2f1a1d350443f90fa6474fd1a4961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: blood\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bebd8e68b1e4cbdac026f04caaf31db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: low_brightness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64992a89ca9429bb726d0eb0a2f7c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38c7236039a4e0aa04b6f97f476442a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: smoke\n",
      "Subdir annotated: data/raw/SegSTRONGC_test/test/9/2\n",
      "Annotating split:  train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92be8055c0444e0080d5c09d9ad1f9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54122827b3bc44ef8a22db4e8f79a519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f097c39fdb6423993960145e720c3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n",
      "Subdir annotated: data/raw/SegSTRONGC_train/train/3/0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9891f487654b62b8425c3f89d59f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afe09bf827743638a91dd844ef4c906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n",
      "Subdir annotated: data/raw/SegSTRONGC_train/train/3/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864a27f16248402794ef36ec1d4b1826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9394b55ff5b6428f9b85ef3796224fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n",
      "Subdir annotated: data/raw/SegSTRONGC_train/train/4/0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b5f0a9e6d145ad9cee105405c435c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65db88c801d3436daa25146501071817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n",
      "Subdir annotated: data/raw/SegSTRONGC_train/train/4/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c94484393a4197ab577cd1ee9345ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c52046176440b9b2e7d1ca08157f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n",
      "Subdir annotated: data/raw/SegSTRONGC_train/train/4/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac7319ff3534b32891fcf26fda15d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eec361863ce426b87970b2a12f9e79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n",
      "Subdir annotated: data/raw/SegSTRONGC_train/train/5/0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619d66fb5d6a41df842f3fc1c01f38f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c52e7c007c4c189fc03a722b3eb986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n",
      "Subdir annotated: data/raw/SegSTRONGC_train/train/5/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce32c2beac70412888673adb42bdbdde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773f84d26013404383c5c3f8ad11f28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n",
      "Subdir annotated: data/raw/SegSTRONGC_train/train/7/0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2c097a61674885922291141ebe726d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd1a5645c474eb6a63af6b97f66008c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n",
      "Subdir annotated: data/raw/SegSTRONGC_train/train/7/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c2584c769e4e7ea770467af855c511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428a128b7d8c41309adfa3bd58b8d273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n",
      "Subdir annotated: data/raw/SegSTRONGC_train/train/8/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcf12c2fbc54c77a3527671eebcfa9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84287d54935e48edb8c9447ea996de4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain annotated: regular\n",
      "Subdir annotated: data/raw/SegSTRONGC_train/train/8/2\n"
     ]
    }
   ],
   "source": [
    "if REFRESH_PROMPTS:\n",
    "    if PROMPTING_STRATEGY == \"first\":\n",
    "        if \"val\" in SPLITS_TO_RUN:\n",
    "            annotate_frames(val_video_folders_path, VAL_DOMAINS, \"val\", 2)\n",
    "        if \"test\" in SPLITS_TO_RUN:\n",
    "            annotate_frames(test_video_folders_path, TEST_DOMAINS, \"test\", 2)\n",
    "        if \"train\" in SPLITS_TO_RUN:\n",
    "            annotate_frames(train_video_folders_path, TRAIN_DOMAINS, \"train\", 2)\n",
    "    elif PROMPTING_STRATEGY == \"all\":\n",
    "        if \"val\" in SPLITS_TO_RUN:\n",
    "            annotate_frames(val_video_folders_path, VAL_DOMAINS, \"val\", TOTAL_FRAMES_PER_VIDEO)\n",
    "        if \"test\" in SPLITS_TO_RUN:\n",
    "            annotate_frames(test_video_folders_path, TEST_DOMAINS, \"test\", TOTAL_FRAMES_PER_VIDEO)\n",
    "        if \"train\" in SPLITS_TO_RUN:\n",
    "            annotate_frames(train_video_folders_path, TRAIN_DOMAINS, \"train\", TOTAL_FRAMES_PER_VIDEO)\n",
    "    elif PROMPTING_STRATEGY == \"dynamic\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompting strategy\")\n",
    "    \n",
    "    for split in SPLITS_TO_RUN:\n",
    "        if SHOULD_VISUALIZE_PROMPTS:\n",
    "            all_annotations = {}\n",
    "            annotation_file = PROMPTS_FOLDER + f\"/{split}.json\"\n",
    "            with open(annotation_file, \"r\") as f:\n",
    "                all_annotations = json.load(f)\n",
    "\n",
    "            annotation_visualization_path = PROMPTS_FOLDER + f\"/{split}_visualization\"\n",
    "\n",
    "            os.makedirs(annotation_visualization_path, exist_ok=True)\n",
    "\n",
    "            for path in all_annotations:\n",
    "                for frame in all_annotations[path]:\n",
    "                    annotations = all_annotations[path][frame]\n",
    "                    ground_truth_path = path + \"/\" + str(frame) + \".png\"\n",
    "\n",
    "                    for domain in VAL_DOMAINS:\n",
    "                        if domain in ground_truth_path:\n",
    "                            ground_truth_path = ground_truth_path.replace(domain, \"ground_truth\")\n",
    "                            break\n",
    "                    \n",
    "                    # print(\"GROUNDDDD TRUTH PATH: \", ground_truth_path)\n",
    "                    ground_truth_image = cv2.imread(ground_truth_path)\n",
    "                    # print(\"ground_truth_image.shape: \", ground_truth_image.shape)\n",
    "                    for tool in annotations:\n",
    "                        if \"bbox\" in annotations[tool]:\n",
    "                            box = annotations[tool][\"bbox\"][\"box\"]\n",
    "                            cv2.rectangle(ground_truth_image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "                            conf = annotations[tool][\"bbox\"][\"conf\"]\n",
    "                            cv2.putText(ground_truth_image, f\"obj-{tool} conf: {conf:.2f}\", (box[0], box[1]-15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                        elif \"points\" in annotations[tool]:\n",
    "                            points = annotations[tool][\"points\"]\n",
    "                            labels = annotations[tool][\"labels\"]\n",
    "                            for point, label in zip(points, labels):\n",
    "                                if tool == 0:\n",
    "                                    if label == 1:\n",
    "                                        cv2.circle(ground_truth_image, (point[0], point[1]), 10, (0, 255, 0), -1)\n",
    "                                    else:\n",
    "                                        cv2.circle(ground_truth_image, (point[0], point[1]), 10, (0, 0, 255), -1)\n",
    "                                else:\n",
    "                                    if label == 1:\n",
    "                                        cv2.circle(ground_truth_image, (point[0], point[1]), 10, (255, 0, 0), -1)\n",
    "                                    else:\n",
    "                                        cv2.circle(ground_truth_image, (point[0], point[1]), 10, (255, 255, 0), -1)\n",
    "                        else:\n",
    "                            print(\"Sadge, no annotations found for this tool: \", tool, \"ground truth path: \", ground_truth_path)\n",
    "\n",
    "                    infix_path = path.split(f\"SegSTRONGC_{split}/\")[-1]\n",
    "                    final_path = annotation_visualization_path + \"/\" + infix_path + \"/\" + str(frame) + \".jpg\"\n",
    "\n",
    "                    os.makedirs(os.path.dirname(final_path), exist_ok=True)\n",
    "                    cv2.imwrite(annotation_visualization_path + \"/\" + infix_path + \"/\" + str(frame) + \".jpg\", ground_truth_image)\n",
    "else:\n",
    "    print(\"Annotations already exist. Skipping annotation process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )\n",
    "\n",
    "sam2_predictor = build_sam2_video_predictor(inference_model_config, inference_model_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(inference_model, frames_path, split, is_reverse, forward_pass_path=None):\n",
    "    mask_storage_data = {}\n",
    "    predicted_masks = []\n",
    "    \n",
    "    if inference_model == \"sam2.1_hiera_base_plus\":\n",
    "        print(f\"Loading annotations for split: {split}\")\n",
    "        try:\n",
    "            with open(PROMPTS_FOLDER + f\"/{split}.json\", \"r\") as f:\n",
    "                annotations = json.load(f)\n",
    "            print(f\"Successfully loaded annotations for {len(annotations)} items\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading annotations: {e}\")\n",
    "            LOGGER.error(f\"Error loading annotations for split {split}: {e}\")\n",
    "            return\n",
    " \n",
    "        if annotations is None:\n",
    "            print(\"No annotations found for split\", split)\n",
    "            LOGGER.warning(f\"No annotations found for split {split}\")\n",
    "            return None, None\n",
    "        \n",
    "        start = time.time()\n",
    "        print(f\"Initializing SAM for video...\")\n",
    "\n",
    "        inference_state = sam2_predictor.init_state(\n",
    "            video_path = frames_path,\n",
    "        )\n",
    "        end = time.time()\n",
    "        print(f\"Initialization took {end - start:.2f} seconds.\")\n",
    "        LOGGER.info(f\"SAM initialization for {forward_pass_path} took {end - start:.2f} seconds.\")\n",
    "\n",
    "        current_annotations = annotations[forward_pass_path]\n",
    "        print(f\"Found {len(current_annotations)} objects with annotations.\")\n",
    "        LOGGER.info(f\"Processing {len(current_annotations)} objects with annotations for {forward_pass_path}\")\n",
    "        \n",
    "        n_points = 0\n",
    "        n_boxes = 0\n",
    "\n",
    "        step = DYNAMIC_FRAME_SKIP if PROMPTING_STRATEGY == \"dynamic\" else 1\n",
    "        frame_range = range(0, TOTAL_FRAMES_PER_VIDEO, step)\n",
    "\n",
    "        for frame in tqdm(frame_range):\n",
    "            if PROMPTING_STRATEGY == \"first\" and frame > 0:\n",
    "                break\n",
    "\n",
    "            # print(f\"Processing annotations for frame {frame}...\")\n",
    "            # print(len(current_annotations), \" annotations found.\")\n",
    "            # # print keys in frame_annotations\n",
    "            # print(\"KEYSSSSS: \", current_annotations.keys())\n",
    "\n",
    "            if is_reverse:\n",
    "                frame_annotations = current_annotations[str(TOTAL_FRAMES_PER_VIDEO - frame - 1)]\n",
    "            else:\n",
    "                frame_annotations = current_annotations[str(frame)]\n",
    "\n",
    "            count = 0\n",
    "            for tool in frame_annotations:\n",
    "                if \"bbox\" in frame_annotations[tool]:\n",
    "                    count += 1\n",
    "\n",
    "            for tool in frame_annotations:\n",
    "                if \"bbox\" in frame_annotations[tool]:\n",
    "                    if count != 2:\n",
    "                        continue\n",
    "                    n_boxes += 1\n",
    "                    box = np.array(frame_annotations[tool][\"bbox\"][\"box\"], dtype=np.float32)\n",
    "                    _, out_obj_ids, out_mask_logits = sam2_predictor.add_new_points_or_box(\n",
    "                        inference_state = inference_state,\n",
    "                        frame_idx = frame,\n",
    "                        obj_id = int(tool),\n",
    "                        box = box\n",
    "                    )\n",
    "                elif \"points\" in frame_annotations[tool]:\n",
    "                    n_points += len(frame_annotations[tool][\"points\"])\n",
    "                    points = np.array(frame_annotations[tool][\"points\"], dtype=np.float32)\n",
    "                    labels = np.array(frame_annotations[tool][\"labels\"], dtype=np.int32)\n",
    "\n",
    "                    _, out_obj_ids, out_mask_logits = sam2_predictor.add_new_points_or_box(\n",
    "                        inference_state = inference_state,\n",
    "                        frame_idx = frame,\n",
    "                        obj_id = int(tool),\n",
    "                        points = points,\n",
    "                        labels = labels\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"No annotations found for object\", tool, \" in frame \", frame)\n",
    "                    LOGGER.warning(f\"No annotations found for object {tool} in frame {frame}\")\n",
    "\n",
    "        print(f\"Added {n_points} annotation points across all objects.\")\n",
    "        LOGGER.info(f\"Added {n_points} annotation points across all objects for {forward_pass_path}\")\n",
    "\n",
    "        print(f\"Added {n_boxes} annotation boxes across all objects.\")\n",
    "        LOGGER.info(f\"Added {n_boxes} annotation boxes across all objects for {forward_pass_path}\")\n",
    "        print(f\"Total annotations: {n_points + n_boxes}\")\n",
    "        LOGGER.info(f\"Total annotations: {n_points + n_boxes} for {forward_pass_path}\")\n",
    "\n",
    "        print(\"Starting mask propagation...\")\n",
    "        start = time.time()\n",
    "        video_segments = {}\n",
    "\n",
    "        n_frames = 0\n",
    "        infix_path = frames_path.split(f\"SegSTRONGC_{split}/\")[-1]\n",
    "\n",
    "        for out_frame_idx, out_obj_ids, out_mask_logits in sam2_predictor.propagate_in_video(\n",
    "            inference_state, \n",
    "            save_mask_logits = SAVE_RUN_MASK_LOGITS, \n",
    "            infix_path = infix_path\n",
    "        ):\n",
    "            n_frames += 1\n",
    "            video_segments[out_frame_idx] = {\n",
    "                out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "                for i, out_obj_id in enumerate(out_obj_ids)\n",
    "            }\n",
    "\n",
    "        end = time.time()\n",
    "        prop_time = end - start\n",
    "\n",
    "        print(f\"Mask propagation completed in {prop_time:.2f} seconds.\")\n",
    "        LOGGER.info(f\"Mask propagation for {forward_pass_path} took {prop_time:.2f} seconds.\")\n",
    "\n",
    "        print(\"Processing predicted masks...\")\n",
    "        for frame_idx, obj_dict in tqdm(video_segments.items(), desc=\"Processing video frames\"):\n",
    "            # it should have shape (1080, 1920)\n",
    "            # mask_storage_data[frame_idx] = []\n",
    "            overall_mask = np.zeros((1080, 1920), dtype=bool)\n",
    "\n",
    "            for obj_id, mask_array in obj_dict.items():\n",
    "                # mask_storage_data[frame_idx].append({\n",
    "                #     obj_id: mask_array\n",
    "                # })\n",
    "                \n",
    "                overall_mask = np.logical_or(overall_mask, mask_array.squeeze())\n",
    "\n",
    "            predicted_masks.append(overall_mask)\n",
    "            mask_storage_data[frame_idx] = overall_mask\n",
    "\n",
    "        sam2_predictor.reset_state(inference_state)\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"Empied CUDA cache.\")\n",
    "        print(\"SAM state reset.\")\n",
    "    elif inference_model == \"yolo11x-seg\":\n",
    "        start = time.time()\n",
    "        print(f\"Initializing Yolo for video...\")\n",
    "        yolo_model = YOLO(inference_model_checkpoint)\n",
    "        end = time.time()\n",
    "        print(f\"Initialization took {end - start:.2f} seconds.\")\n",
    "        LOGGER.info(f\"Yolo initialization for {frames_path} took {end - start:.2f} seconds.\")\n",
    "\n",
    "        print(\"Starting mask propagation...\")\n",
    "        start = time.time()\n",
    "\n",
    "        total_images = len(os.listdir(frames_path))\n",
    "        print(f\"Total images in video: {total_images}\")\n",
    "        LOGGER.info(f\"Total images in video: {total_images}\")\n",
    "\n",
    "        video_result = []\n",
    "\n",
    "        for i in tqdm(range(total_images), desc=\"Propagating video frames\"):\n",
    "            frame_path = frames_path + f\"/{i}.png\"\n",
    "            # video_result.append(yolo_model(frame_path, max_det = 2))\n",
    "            video_result.append(yolo_model(frame_path))\n",
    "\n",
    "        end = time.time()\n",
    "        prop_time = end - start\n",
    "\n",
    "        print(f\"Mask propagation completed in {prop_time:.2f} seconds.\")\n",
    "        LOGGER.info(f\"Mask propagation for {frames_path} took {prop_time:.2f} seconds.\")\n",
    "\n",
    "        print(\"Processing predicted masks...\")\n",
    "\n",
    "        for frame_idx, frame_result in tqdm(enumerate(video_result), desc=\"Processing video frames\"):\n",
    "            # it should have shape (1080, 1920)\n",
    "            # mask_storage_data[frame_idx] = []\n",
    "            overall_mask = np.zeros((1080, 1920), dtype=bool)\n",
    "\n",
    "            for result in frame_result:\n",
    "                if result.masks is None:\n",
    "                    # mask_storage_data[frame_idx].append({\n",
    "                    #     0: overall_mask\n",
    "                    # })\n",
    "                    continue\n",
    "\n",
    "                for mask_id, mask in enumerate(result.masks.data):\n",
    "                    mask_np = mask.cpu().numpy()\n",
    "                    reshaped_mask = cv2.resize(mask_np, (1920, 1080), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                    # mask_storage_data[frame_idx].append({\n",
    "                    #     mask_id: reshaped_mask\n",
    "                    # })\n",
    "\n",
    "                    overall_mask = np.logical_or(overall_mask, reshaped_mask)\n",
    "\n",
    "            predicted_masks.append(overall_mask)\n",
    "            mask_storage_data[frame_idx] = overall_mask\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name.\")\n",
    "    \n",
    "    if SAVE_OUTPUT_MASKS:\n",
    "        for i in range(len(predicted_masks)):\n",
    "            infix_path = frames_path.split(f\"SegSTRONGC_{split}/\")[-1]\n",
    "            final_path = BASE_RESULTS_DIR + f\"/{inference_model}/demo/\" + infix_path + \"/\" + str(i) + \".jpg\"\n",
    "            os.makedirs(os.path.dirname(final_path), exist_ok=True)\n",
    "            cv2.imwrite(final_path, predicted_masks[i] * 255)\n",
    "\n",
    "    return mask_storage_data, predicted_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (4039668190.py, line 157)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[29], line 157\u001b[0;36m\u001b[0m\n\u001b[0;31m    if PR\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "def process_video(frames_path, sub_dir, domain, split, is_left):\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Processing video: {frames_path}\")\n",
    "    print(f\"Domain: {domain}, Split: {split}, Camera: {'left' if is_left else 'right'}\")\n",
    "    LOGGER.info(f\"Processing video: {frames_path} (Domain: {domain}, Split: {split}, Camera: {'left' if is_left else 'right'})\")\n",
    "    stereo_dir = \"left\" if is_left else \"right\"\n",
    "    ground_truth_masks_path = sub_dir + \"/ground_truth/\" + stereo_dir\n",
    "\n",
    "    overall_start = time.time()\n",
    "    mask_storage_data, predicted_masks = run_inference(INFERENCE_MODEL, frames_path, split, False, frames_path)\n",
    "\n",
    "    if SHOULD_PERFROM_CYCLIC_TTA:\n",
    "        temp_video_frames_path = \"data/temp\"\n",
    "        if not os.path.exists(temp_video_frames_path):\n",
    "            os.makedirs(temp_video_frames_path)\n",
    "\n",
    "        total_files = len(os.listdir(frames_path))\n",
    "        for filename in os.listdir(frames_path):\n",
    "            if filename.endswith(\".png\"):\n",
    "                original_index = int(filename.split('.')[0])\n",
    "                new_index = total_files - 1 - original_index\n",
    "                new_filename = f\"{new_index}.png\"\n",
    "                shutil.copy(os.path.join(frames_path, filename), os.path.join(temp_video_frames_path, new_filename))\n",
    "\n",
    "        reverse_mask_storage_data, predicted_reverse_masks = run_inference(INFERENCE_MODEL, temp_video_frames_path, split, True, frames_path)\n",
    "        predicted_reverse_masks = predicted_reverse_masks[::-1]\n",
    "\n",
    "        if SAVE_IMAGES_ONCE:\n",
    "            save_dir = f\"data/results/{INFERENCE_MODEL}/visualizations\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            for i in range(len(predicted_masks)):\n",
    "                # Get frame path and load original image\n",
    "                frame_path = os.path.join(frames_path, f\"{i}.png\")\n",
    "                original_img = cv2.imread(frame_path)\n",
    "                \n",
    "                # Load ground truth mask\n",
    "                gt_mask = cv2.imread(os.path.join(ground_truth_masks_path, f\"{i}.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "                gt_mask = (gt_mask > 0).astype(np.uint8) * 255\n",
    "                \n",
    "                # Convert predicted masks to uint8\n",
    "                forward_mask = predicted_masks[i].astype(np.uint8) * 255\n",
    "                reverse_mask = predicted_reverse_masks[i].astype(np.uint8) * 255\n",
    "                \n",
    "                # Create visualization grid\n",
    "                h, w = original_img.shape[:2]\n",
    "                grid = np.zeros((h*2, w*2, 3), dtype=np.uint8)\n",
    "                \n",
    "                # Place images in grid\n",
    "                grid[:h, :w] = original_img  # Original\n",
    "                grid[:h, w:] = cv2.cvtColor(gt_mask, cv2.COLOR_GRAY2BGR)  # Ground truth\n",
    "                grid[h:, :w] = cv2.cvtColor(forward_mask, cv2.COLOR_GRAY2BGR)  # Forward mask\n",
    "                grid[h:, w:] = cv2.cvtColor(reverse_mask, cv2.COLOR_GRAY2BGR)  # Reverse mask\n",
    "                \n",
    "                # Add labels\n",
    "                labels = ['Original', 'Ground Truth', 'Forward Mask', 'Reverse Mask']\n",
    "                positions = [(10, 30), (w+10, 30), (10, h+30), (w+10, h+30)]\n",
    "                \n",
    "                for label, pos in zip(labels, positions):\n",
    "                    cv2.putText(grid, label, pos, cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "                \n",
    "                # Save the grid\n",
    "                save_path = os.path.join(save_dir, f\"frame_{i:04d}.png\")\n",
    "                cv2.imwrite(save_path, grid)\n",
    "                \n",
    "                print(f\"Saved visualization images to {save_dir}\")\n",
    "\n",
    "        SAVE_IMAGES_ONCE = False\n",
    "\n",
    "        for i in range(len(predicted_masks)):\n",
    "            predicted_masks[i] = np.logical_or(predicted_masks[i], predicted_reverse_masks[i])\n",
    "\n",
    "        shutil.rmtree(temp_video_frames_path)\n",
    "        print(f\"Applied TTA to {len(predicted_masks)} masks.\")\n",
    "    \n",
    "    print(f\"Generated {len(predicted_masks)} masks for evaluation.\")\n",
    "    LOGGER.info(f\"Generated {len(predicted_masks)} masks for {frames_path}\")\n",
    "\n",
    "    # Save the masks\n",
    "    masks_split_dir = MASKS_DIR + f\"/{INFERENCE_MODEL}\" + f\"/{split}\"\n",
    "    if not os.path.exists(masks_split_dir):\n",
    "        os.makedirs(masks_split_dir)\n",
    "\n",
    "    # masks_file = masks_split_dir + f\"/{frames_path.replace('/', '-')}.pkl\"\n",
    "\n",
    "    # data = {}\n",
    "    # data[frames_path] = mask_storage_data\n",
    "\n",
    "    # with open(masks_file, \"wb\") as f:\n",
    "    #     pickle.dump(data, f)\n",
    "    # print(f\"Masks saved to {masks_file}\")\n",
    "\n",
    "    print(\"Loading ground truth masks for evaluation...\")\n",
    "    ground_truth_masks = []\n",
    "    for i in range(len(predicted_masks)):\n",
    "        ground_truth_mask = cv2.imread(ground_truth_masks_path + \"/\" + str(i) + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "        ground_truth_mask = (ground_truth_mask > 0).astype(np.bool_)\n",
    "        ground_truth_masks.append(ground_truth_mask)\n",
    "    print(f\"Loaded {len(ground_truth_masks)} ground truth masks.\")\n",
    "\n",
    "    print(\"Calculating evaluation metrics...\")\n",
    "    start = time.time()\n",
    "    miou = calculate_miou(predicted_masks, ground_truth_masks)\n",
    "    mdsc = calculate_mdsc(predicted_masks, ground_truth_masks)\n",
    "    end = time.time()\n",
    "    eval_time = end - start\n",
    "    print(f\"Time taken for metrics calculation: {eval_time:.2f} seconds.\")\n",
    "    LOGGER.info(f\"Time taken for metrics calculation: {eval_time:.2f} seconds.\")\n",
    "\n",
    "    print(f\"Mean IoU for {sub_dir}/{domain}/{stereo_dir}: {miou:.4f}\")\n",
    "    print(f\"Mean DSC for {sub_dir}/{domain}/{stereo_dir}: {mdsc:.4f}\")\n",
    "\n",
    "    LOGGER.info(f\"Mean IoU for {sub_dir}/{domain}/{stereo_dir}: {miou:.4f}\")\n",
    "    LOGGER.info(f\"Mean DSC for {sub_dir}/{domain}/{stereo_dir}: {mdsc:.4f}\")\n",
    "\n",
    "    results_file = BASE_RESULTS_DIR + f\"/{INFERENCE_MODEL}\" + f\"/{split}.json\"\n",
    "    if os.path.exists(results_file):\n",
    "        print(f\"Loading existing results file: {results_file}\")\n",
    "        with open(results_file, \"r\") as f:\n",
    "            all_results = json.load(f)\n",
    "    else:\n",
    "        print(f\"Creating new results file: {results_file}\")\n",
    "        all_results = {}\n",
    "    \n",
    "    all_results[frames_path] = {\n",
    "        \"miou\": miou,\n",
    "        \"mdsc\": mdsc\n",
    "    }\n",
    "\n",
    "    with open(results_file, \"w\") as f:\n",
    "        json.dump(all_results, f)\n",
    "    print(f\"Results saved to {results_file}\")\n",
    "\n",
    "    overall_end = time.time()\n",
    "    total_time = overall_end - overall_start\n",
    "    print(f\"Processing video took {total_time:.2f} seconds.\")\n",
    "    LOGGER.info(f\"Results for {sub_dir}/{domain}/{stereo_dir} saved.\")\n",
    "    LOGGER.info(f\"Processing video took {total_time:.2f} seconds.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return miou, mdsc\n",
    "\n",
    "def process_split(sub_dirs, domains, split):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Running inference for split: {split}\")\n",
    "    LOGGER.info(f\"Using Model: {INFERENCE_MODEL}\")\n",
    "    LOGGER.info(f\"Annotation mode: {'manual' if SHOULD_USE_MANUAL_PROMPT else 'auto'}\")\n",
    "    LOGGER.info(f\"Performing tta: {'yes' if SHOULD_PERFROM_CYCLIC_TTA else 'no'}\")\n",
    "    LOGGER.info(f\"Using box prompt: {'yes' if SHOULD_USE_BOX_PROMPT else 'no'}\")\n",
    "    LOGGER.info(f\"Using ground truth for box prompt: {'yes' if SHOULD_SAMPLE_GROUND_TRUTH else 'no'}\")\n",
    "    LOGGER.info(f\"Refreshing prompts: {'yes' if REFRESH_PROMPTS else 'no'}\")\n",
    "    LOGGER.info(f\"Prompting strategy: {PROMPTING_STRATEGY}\")\n",
    "    if PROMPTING_STRATEGY == \"dynamic\":\n",
    "        LOGGER.info(f\"Dynamic frame skip: {DYNAMIC_FRAME_SKIP}\")\n",
    "    LOGGER.info(f\"Saving run mask logits: {'yes' if SAVE_RUN_MASK_LOGITS else 'no'}\")\n",
    "    LOGGER.info(f\"Saving images once: {'yes' if SAVE_IMAGES_ONCE else 'no'}\")\n",
    "    LOGGER.info(f\"Visualizing prompts: {'yes' if SHOULD_VISUALIZE_PROMPTS else 'no'}\")\n",
    "    \n",
    "    if not SHOULD_USE_BOX_PROMPT:\n",
    "        LOGGER.info(f\"Number of positive point annotations per tool: {NUM_POS_POINTS_PER_TOOL}\")\n",
    "        LOGGER.info(f\"Number of negative point annotations per tool: {NUM_NEG_POINTS_PER_TOOL}\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    LOGGER.info(f\"----------------Running inference for split {split}-------------\")\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    print(f\"Processing {len(sub_dirs)} sub-directories and {len(domains)} domains\")\n",
    "    LOGGER.info(f\"Processing {len(sub_dirs)} sub-directories and {len(domains)} domains for split {split}\")\n",
    "        \n",
    "    sub_dir_results = {}\n",
    "    for sub_dir in tqdm(sub_dirs, desc=f\"Processing sub-directories\"):\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(f\"Processing sub-directory: {sub_dir}\")\n",
    "        LOGGER.info(f\"Processing sub-directory: {sub_dir}\")\n",
    "        domain_results = {}\n",
    "        for domain in tqdm(domains, desc=f\"Processing domains\"):\n",
    "            print(f\"\\nProcessing domain: {domain}\")\n",
    "            LOGGER.info(f\"Processing domain: {domain} in {sub_dir}\")\n",
    "            left_video_frames_path = sub_dir + \"/\" + domain + \"/left\"\n",
    "            right_video_frames_path = sub_dir + \"/\" + domain + \"/right\"\n",
    "\n",
    "            print(f\"Processing left camera video...\")\n",
    "            left_miou, left_msdc = process_video(left_video_frames_path, sub_dir, domain, split, True)\n",
    "            \n",
    "            print(f\"Processing right camera video...\")\n",
    "            right_miou, right_msdc = process_video(right_video_frames_path, sub_dir, domain, split, False)\n",
    "\n",
    "            overall_miou = (left_miou + right_miou) / 2\n",
    "            overall_msdc = (left_msdc + right_msdc) / 2\n",
    "\n",
    "            print(f\"\\nResults for {sub_dir}/{domain}:\")\n",
    "            print(f\"  Left: IoU={left_miou:.4f}, DSC={left_msdc:.4f}\")\n",
    "            print(f\"  Right: IoU={right_miou:.4f}, DSC={right_msdc:.4f}\")\n",
    "            print(f\"  Overall: IoU={overall_miou:.4f}, DSC={overall_msdc:.4f}\")\n",
    "            \n",
    "            LOGGER.info(f\"Results for {sub_dir}/{domain}: Left IoU={left_miou:.4f}, Right IoU={right_miou:.4f}, Overall IoU={overall_miou:.4f}\")\n",
    "\n",
    "            domain_results[domain] = {\n",
    "                \"left_miou\": left_miou,\n",
    "                \"left_msdc\": left_msdc,\n",
    "                \"right_miou\": right_miou,\n",
    "                \"right_msdc\": right_msdc,\n",
    "                \"overall_miou\": overall_miou,\n",
    "                \"overall_msdc\": overall_msdc\n",
    "            }\n",
    "\n",
    "        sub_dir_results[sub_dir] = domain_results\n",
    "\n",
    "    content = \"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"SUMMARY RESULTS FOR SPLIT: {split}\")\n",
    "    print(\"=\"*60)\n",
    "    LOGGER.info(f\"SUMMARY RESULTS FOR SPLIT: {split}\")\n",
    "    LOGGER.info(f'Description: {RUN_DESCRIPTION}')\n",
    "    LOGGER.info(f\"Using Model: {INFERENCE_MODEL}\")\n",
    "    LOGGER.info(f\"Annotation mode: {'manual' if SHOULD_USE_MANUAL_PROMPT else 'auto'}\")\n",
    "    LOGGER.info(f\"Performing tta: {'yes' if SHOULD_PERFROM_CYCLIC_TTA else 'no'}\")\n",
    "    LOGGER.info(f\"Using box prompt: {'yes' if SHOULD_USE_BOX_PROMPT else 'no'}\")\n",
    "    LOGGER.info(f\"Using ground truth for box prompt: {'yes' if SHOULD_SAMPLE_GROUND_TRUTH else 'no'}\")\n",
    "    LOGGER.info(f\"Refreshing prompts: {'yes' if REFRESH_PROMPTS else 'no'}\")\n",
    "    LOGGER.info(f\"Prompting strategy: {PROMPTING_STRATEGY}\")\n",
    "    if PROMPTING_STRATEGY == \"dynamic\":\n",
    "        LOGGER.info(f\"Dynamic frame skip: {DYNAMIC_FRAME_SKIP}\")\n",
    "    LOGGER.info(f\"Saving run mask logits: {'yes' if SAVE_RUN_MASK_LOGITS else 'no'}\")\n",
    "    LOGGER.info(f\"Saving images once: {'yes' if SAVE_IMAGES_ONCE else 'no'}\")\n",
    "    LOGGER.info(f\"Visualizing prompts: {'yes' if SHOULD_VISUALIZE_PROMPTS else 'no'}\")\n",
    "    \n",
    "    if not SHOULD_USE_BOX_PROMPT:\n",
    "        LOGGER.info(f\"Number of positive point annotations per tool: {NUM_POS_POINTS_PER_TOOL}\")\n",
    "        LOGGER.info(f\"Number of negative point annotations per tool: {NUM_NEG_POINTS_PER_TOOL}\")\n",
    "\n",
    "\n",
    "    content += f\"SUMMARY RESULTS FOR SPLIT: {split}\\n\"\n",
    "    content += f\"Description: {RUN_DESCRIPTION}\\n\"\n",
    "    content += f\"Using Model: {INFERENCE_MODEL}\\n\"\n",
    "    content += f\"Annotation mode: {'manual' if SHOULD_USE_MANUAL_PROMPT else 'auto'}\\n\"\n",
    "    content += f\"Performing tta: {'yes' if SHOULD_PERFROM_CYCLIC_TTA else 'no'}\\n\"\n",
    "    content += f\"Using box prompt: {'yes' if SHOULD_USE_BOX_PROMPT else 'no'}\\n\"\n",
    "    content += f\"Using ground truth for box prompt: {'yes' if SHOULD_SAMPLE_GROUND_TRUTH else 'no'}\\n\"\n",
    "    content += f\"Refreshing prompts: {'yes' if REFRESH_PROMPTS else 'no'}\\n\"\n",
    "    content += f\"Prompting strategy: {PROMPTING_STRATEGY}\\n\"\n",
    "    if PROMPTING_STRATEGY == \"dynamic\":\n",
    "        content += f\"Dynamic frame skip: {DYNAMIC_FRAME_SKIP}\\n\"\n",
    "    content += f\"Saving run mask logits: {'yes' if SAVE_RUN_MASK_LOGITS else 'no'}\\n\"\n",
    "    content += f\"Saving images once: {'yes' if SAVE_IMAGES_ONCE else 'no'}\\n\"\n",
    "    content += f\"Visualizing prompts: {'yes' if SHOULD_VISUALIZE_PROMPTS else 'no'}\\n\"\n",
    "\n",
    "    if not SHOULD_USE_BOX_PROMPT:\n",
    "        content += f\"Number of positive point annotations per tool: {NUM_POS_POINTS_PER_TOOL}\\n\"\n",
    "        content += f\"Number of negative point annotations per tool: {NUM_NEG_POINTS_PER_TOOL}\\n\"\n",
    "\n",
    "    # Domain-wise results\n",
    "    print(\"\\nDomain-wise Results:\")\n",
    "    LOGGER.info(\"Domain-wise Results:\")\n",
    "\n",
    "    content += \"\\nDomain-wise Results:\\n\"\n",
    "    domain_results_data = {}\n",
    "    for domain in domains:\n",
    "        left_mious = [sub_dir_results[sub_dir][domain][\"left_miou\"] for sub_dir in sub_dirs]\n",
    "        right_mious = [sub_dir_results[sub_dir][domain][\"right_miou\"] for sub_dir in sub_dirs]\n",
    "        overall_mious = [sub_dir_results[sub_dir][domain][\"overall_miou\"] for sub_dir in sub_dirs]\n",
    "\n",
    "        left_msdcs = [sub_dir_results[sub_dir][domain][\"left_msdc\"] for sub_dir in sub_dirs]\n",
    "        right_msdcs = [sub_dir_results[sub_dir][domain][\"right_msdc\"] for sub_dir in sub_dirs]\n",
    "        overall_msdcs = [sub_dir_results[sub_dir][domain][\"overall_msdc\"] for sub_dir in sub_dirs]\n",
    "\n",
    "        print(f\"\\nDomain: {domain}\")\n",
    "        print(f\"  Left Frame IoU: {np.mean(left_mious):.4f}\")\n",
    "        print(f\"  Right Frame IoU: {np.mean(right_mious):.4f}\")\n",
    "        print(f\"  Overall IoU: {np.mean(overall_mious):.4f}\")\n",
    "        print(f\"  Left Frame DSC: {np.mean(left_msdcs):.4f}\")\n",
    "        print(f\"  Right Frame DSC: {np.mean(right_msdcs):.4f}\")\n",
    "        print(f\"  Overall DSC: {np.mean(overall_msdcs):.4f}\")\n",
    "        \n",
    "        LOGGER.info(f\"Domain {domain} - Left IoU: {np.mean(left_mious):.4f}, Right IoU: {np.mean(right_mious):.4f}, Overall IoU: {np.mean(overall_mious):.4f}\")\n",
    "        LOGGER.info(f\"Domain {domain} - Left DSC: {np.mean(left_msdcs):.4f}, Right DSC: {np.mean(right_msdcs):.4f}, Overall DSC: {np.mean(overall_msdcs):.4f}\")\n",
    "\n",
    "        content += f\"\\nDomain: {domain}\\n\"\n",
    "        content += f\"  Left Frame IoU: {np.mean(left_mious):.4f}\\n\"\n",
    "        content += f\"  Right Frame IoU: {np.mean(right_mious):.4f}\\n\"\n",
    "        content += f\"  Overall IoU: {np.mean(overall_mious):.4f}\\n\"\n",
    "        content += f\"  Left Frame DSC: {np.mean(left_msdcs):.4f}\\n\"\n",
    "        content += f\"  Right Frame DSC: {np.mean(right_msdcs):.4f}\\n\"\n",
    "        content += f\"  Overall DSC: {np.mean(overall_msdcs):.4f}\\n\"\n",
    "\n",
    "        \n",
    "        domain_results_data[domain] = {\n",
    "            \"left_miou\": np.mean(left_mious),\n",
    "            \"right_miou\": np.mean(right_mious),\n",
    "            \"overall_miou\": np.mean(overall_mious),\n",
    "            \"left_mdsc\": np.mean(left_msdcs),\n",
    "            \"right_mdsc\": np.mean(right_msdcs),\n",
    "            \"overall_mdsc\": np.mean(overall_msdcs)\n",
    "        }\n",
    "\n",
    "    # Overall results across all domains and sub-dirs\n",
    "    left_mious = [np.mean([sub_dir_results[sub_dir][domain][\"left_miou\"] for domain in domains]) for sub_dir in sub_dirs]\n",
    "    right_mious = [np.mean([sub_dir_results[sub_dir][domain][\"right_miou\"] for domain in domains]) for sub_dir in sub_dirs]\n",
    "    overall_mious = [np.mean([sub_dir_results[sub_dir][domain][\"overall_miou\"] for domain in domains]) for sub_dir in sub_dirs]\n",
    "\n",
    "    left_msdcs = [np.mean([sub_dir_results[sub_dir][domain][\"left_msdc\"] for domain in domains]) for sub_dir in sub_dirs]\n",
    "    right_msdcs = [np.mean([sub_dir_results[sub_dir][domain][\"right_msdc\"] for domain in domains]) for sub_dir in sub_dirs]\n",
    "    overall_msdcs = [np.mean([sub_dir_results[sub_dir][domain][\"overall_msdc\"] for domain in domains]) for sub_dir in sub_dirs]\n",
    "\n",
    "    content += \"-------------------------------------------------------------------------\"\n",
    "\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"FINAL RESULTS ACROSS ALL DOMAINS AND SUB-DIRECTORIES:\")\n",
    "    print(f\"  Left Frame IoU: {np.mean(left_mious):.4f}\")\n",
    "    print(f\"  Right Frame IoU: {np.mean(right_mious):.4f}\")\n",
    "    print(f\"  Overall IoU: {np.mean(overall_mious):.4f}\")\n",
    "    print(f\"  Left Frame DSC: {np.mean(left_msdcs):.4f}\")\n",
    "    print(f\"  Right Frame DSC: {np.mean(right_msdcs):.4f}\")\n",
    "    print(f\"  Overall DSC: {np.mean(overall_msdcs):.4f}\")\n",
    "\n",
    "    LOGGER.info(\"FINAL RESULTS ACROSS ALL DOMAINS AND SUB-DIRECTORIES:\")\n",
    "    LOGGER.info(f\"Left Frame IoU: {np.mean(left_mious):.4f}\")\n",
    "    LOGGER.info(f\"Right Frame IoU: {np.mean(right_mious):.4f}\")\n",
    "    LOGGER.info(f\"Overall IoU: {np.mean(overall_mious):.4f}\")\n",
    "    LOGGER.info(f\"Left Frame DSC: {np.mean(left_msdcs):.4f}\")\n",
    "    LOGGER.info(f\"Right Frame DSC: {np.mean(right_msdcs):.4f}\")\n",
    "    LOGGER.info(f\"Overall DSC: {np.mean(overall_msdcs):.4f}\")\n",
    "\n",
    "    content += \"FINAL RESULTS ACROSS ALL DOMAINS AND SUB-DIRECTORIES:\\n\"\n",
    "    content += f\"Left Frame IoU: {np.mean(left_mious):.4f}\\n\"\n",
    "    content += f\"Right Frame IoU: {np.mean(right_mious):.4f}\\n\"\n",
    "    content += f\"Overall IoU: {np.mean(overall_mious):.4f}\\n\"\n",
    "    content += f\"Left Frame DSC: {np.mean(left_msdcs):.4f}\\n\"\n",
    "    content += f\"Right Frame DSC: {np.mean(right_msdcs):.4f}\\n\"\n",
    "    content += f\"Overall DSC: {np.mean(overall_msdcs):.4f}\\n\"\n",
    "\n",
    "    overall_end = time.time()\n",
    "    total_time = overall_end - overall_start\n",
    "    print(f\"\\nTotal time taken for split {split}: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    LOGGER.info(f\"Total time taken for split {split}: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    content += f\"\\nTotal time taken for split {split}: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\\n\"\n",
    "    \n",
    "    # Save results to CSV\n",
    "    \n",
    "    csv_file = f'data/results/results.csv'\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    data = {\n",
    "        'timestamp': timestamp,\n",
    "        'desc': RUN_DESCRIPTION,\n",
    "        'model': INFERENCE_MODEL,\n",
    "        'split': split,\n",
    "        'annotation_mode': 'manual' if SHOULD_USE_MANUAL_PROMPT else 'auto',\n",
    "        'num_pos_points': NUM_POS_POINTS_PER_TOOL if not SHOULD_USE_BOX_PROMPT else 'N/A',\n",
    "        'num_neg_points': NUM_NEG_POINTS_PER_TOOL if not SHOULD_USE_BOX_PROMPT else 'N/A',\n",
    "        'tta': 'yes' if SHOULD_PERFROM_CYCLIC_TTA else 'no',\n",
    "        'overall_left_miou': np.mean(left_mious),\n",
    "        'overall_right_miou': np.mean(right_mious),\n",
    "        'overall_miou': np.mean(overall_mious),\n",
    "        'overall_left_mdsc': np.mean(left_msdcs),\n",
    "        'overall_right_mdsc': np.mean(right_msdcs),\n",
    "        'overall_mdsc': np.mean(overall_msdcs),\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }\n",
    "    \n",
    "    # Add domain-specific results\n",
    "    for domain in domains:\n",
    "        data[f'{domain}_left_miou'] = domain_results_data[domain]['left_miou']\n",
    "        data[f'{domain}_right_miou'] = domain_results_data[domain]['right_miou']\n",
    "        data[f'{domain}_overall_miou'] = domain_results_data[domain]['overall_miou']\n",
    "        data[f'{domain}_left_mdsc'] = domain_results_data[domain]['left_mdsc']\n",
    "        data[f'{domain}_right_mdsc'] = domain_results_data[domain]['right_mdsc']\n",
    "        data[f'{domain}_overall_mdsc'] = domain_results_data[domain]['overall_mdsc']\n",
    "    \n",
    "    # Convert to DataFrame for a single row\n",
    "    df_new = pd.DataFrame([data])\n",
    "    \n",
    "    # Check if file exists and append, or create new\n",
    "    if os.path.exists(csv_file):\n",
    "        df_existing = pd.read_csv(csv_file)\n",
    "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(csv_file), exist_ok=True)\n",
    "        df_new.to_csv(csv_file, index=False)\n",
    "    \n",
    "    print(f\"Results saved to CSV file: {csv_file}\")\n",
    "    LOGGER.info(f\"Results saved to CSV file: {csv_file}\")\n",
    "\n",
    "    requests.post(DISCORD_WEBHOOK_URL, { \"content\": content, \"username\" : f\"{split}-runner\" })\n",
    "\n",
    "for split in SPLITS_TO_RUN:\n",
    "    if \"val\" == split:\n",
    "        print(\"Running inference for validation split\")\n",
    "        requests.post(DISCORD_WEBHOOK_URL, { \"content\": \"Running inference for validation split\", \"username\" : \"val-runner\"  })\n",
    "        process_split(val_video_folders_path, VAL_DOMAINS, \"val\")\n",
    "    elif \"test\" == split:\n",
    "        print(\"Running inference for test split\")\n",
    "        requests.post(DISCORD_WEBHOOK_URL, { \"content\": \"Running inference for test split\", \"username\" : \"test-runner\" })\n",
    "        process_split(test_video_folders_path, TEST_DOMAINS, \"test\")\n",
    "    elif \"train\" == split:\n",
    "        print(\"Running inference for train split\")\n",
    "        requests.post(DISCORD_WEBHOOK_URL, { \"content\": \"Running inference for train split\", \"username\" : \"train-runner\"  })\n",
    "        process_split(train_video_folders_path, TRAIN_DOMAINS, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domains = ['bg_change', 'blood', 'low_brightness', 'regular', 'smoke']\n",
    "# annotations = None\n",
    "# with open('data/annotations/auto/val.json', 'r') as f:\n",
    "#     annotations = json.load(f)\n",
    "\n",
    "# path = \"data/masks/sam2.1_hiera_base_plus/val/data-raw-SegSTRONGC_val-val-1-2-bg_change-right.pkl\"\n",
    "# with open(path, 'rb') as f:\n",
    "#     mass = pickle.load(f)\n",
    "#     for video_path, video_data in mass.items():\n",
    "#         # print(video_path, video_data)\n",
    "#         for frame_id, frame_data in video_data.items():\n",
    "#             overall_mask = np.zeros((1080, 1920), dtype=bool)\n",
    "#             for data in frame_data:\n",
    "#                 for object_id, mask in data.items():\n",
    "#                     overall_mask = np.logical_or(overall_mask, mask[0])\n",
    "\n",
    "#             ground_truth_masks_path = video_path\n",
    "#             for domain in domains:\n",
    "#                 if domain in video_path:\n",
    "#                     ground_truth_masks_path = video_path.replace(domain, 'ground_truth')\n",
    "#                     break\n",
    "#             ground_truth_masks_path = ground_truth_masks_path + \"/\" + str(frame_id) + \".jpg\"\n",
    "#             ground_truth_mask = cv2.imread(ground_truth_masks_path, cv2.IMREAD_GRAYSCALE)\n",
    "#             ground_truth_mask = (ground_truth_mask > 0).astype(np.bool_)\n",
    "\n",
    "#             original_image = cv2.imread(video_path + \"/\" + str(frame_id) + \".jpg\")\n",
    "#             original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#             # frame_annotations = annotations[video_path.replace(\"/left\", \"\").replace(\"/right\", \"\")]\n",
    "#             frame_annotations = annotations[video_path]\n",
    "\n",
    "#             # place dots in the original image for each annotation\n",
    "#             for object_id, object_annotations in frame_annotations.items():\n",
    "#                 # print(object_annotations)\n",
    "#                 for annotation in object_annotations:\n",
    "#                     x = annotation['x']\n",
    "#                     y = annotation['y']\n",
    "#                     label = annotation['label']\n",
    "#                     if object_id == \"0\":\n",
    "#                         original_image = cv2.circle(original_image, (x, y), 10, (0, 255, 0), -1)\n",
    "#                     else:\n",
    "#                         original_image = cv2.circle(original_image, (x, y), 10, (255, 0, 0), -1)\n",
    "\n",
    "#             #show the masks and the original image\n",
    "#             fig, axs = plt.subplots(1, 3, figsize=(30, 15))\n",
    "#             axs[0].imshow(original_image)\n",
    "#             axs[0].set_title(\"Original Image\")\n",
    "#             axs[1].imshow(overall_mask)\n",
    "#             axs[1].set_title(\"Overall Mask\")\n",
    "#             axs[2].imshow(ground_truth_mask)\n",
    "#             axs[2].set_title(\"Ground Truth Mask\")\n",
    "#             plt.show()\n",
    "#             break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
